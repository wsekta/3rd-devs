![](https://cloud.overment.com/S00E02-2-1730634338.png)

# S00E02 — Prompt Engineering

Prompt Engineering po premierze ChatGPT był uznawany za specjalizację "przyszłości", ale rozwój modeli szybko to zakwestionował. Faktycznie poziom rozumienia poleceń czy nawet posługiwania się długim kontekstem jest nieporównywalny z tym, co widzieliśmy wcześniej. Z drugiej strony, jako programiści, nie mówimy tutaj o bezpośredniej rozmowie z LLM, lecz o projektowaniu złożonych systemów zdolnych do częściowo autonomicznego realizowania zadań. Tutaj precyzyjne kształtowanie instrukcji,  składających się z dynamicznych elementów, nadal odgrywa fundamentalną rolę.
 
## Zasady interakcji z LLM

Z poprzedniej lekcji wiemy, że LLM generuje treść, przewidując kolejne tokeny, co można porównać do mechanizmów autouzupełniania (Completion). Jednak z programistycznego punktu widzenia, praca z modelem w takiej formie jest problematyczna. Dlatego OpenAI zasugerowało strukturę o nazwie ChatML, którą nieoficjalnie można uznać za standard, ponieważ pojawia się ona również w przypadku modeli innych firm.

![Strukturyzowanie interakcji z modelem poprzez ChatML](https://cloud.overment.com/2024-08-05/aidevs3_chatml-07bb4523-1.png)

Wiele wskazuje, że kształt ChatML nie jest zdefiniowany ostatecznie, ponieważ potrzeba utrzymywania lepszej kontroli nad zachowaniem modelu prowadzi do wprowadzania kolejnych ról czy rodzajów treści. Pewien kierunek rysuje dokument [Model Spec](https://cdn.openai.com/spec/model-spec-2024-05-08.html) opublikowany przez OpenAI w połowie 2024 roku, jednak trudno powiedzieć w jakiej formie przełoży się on na praktykę.

Tymczasem należy wiedzieć, że podział na role **system, user, assistant** nie zmienia zasady mówiącej o przewidywaniu kolejnego tokenu. Po prostu z punktu widzenia LLM, przetwarzana treść zawiera dodatkowe tokeny strukturyzujące widoczne w Tiktokenizer. 

![Podział treści przetwarzanych przez LLM na tokeny z uwzględnieniem tokenów specjalnych](https://cloud.overment.com/2024-08-05/aidevs_chatmltokens-32f80743-3.png)

Rola **system** zwykle jest niewidoczna dla użytkownika końcowego. Zawiera instrukcje nadające ton konwersacji oraz określające zachowanie asystenta. Może również zawierać wiedzę, którą LLM wykorzystuje w swoich wypowiedziach. Nie można jednak zakładać, że jej treść będzie niemożliwa do przechwycenia, ale tak nie jest, o czym przekonamy się później.

LLM API jest bezstanowe, a zatem **każda kolejna interakcja z modelem rozpoczyna się od początku.** ChatGPT może nam sugerować, że jest inaczej, pozwalając nam na prowadzenie dyskusji. Jest to jednak rezultat mechaniki działającej w tle, która dba o to, aby za każdym razem przekazywać kompletny zestaw informacji niezbędny modelowi do "uzupełnienia dalszej *części* rozmowy". Poniżej widzimy, dwie oddzielne konwersacje, których model nie jest w stanie samodzielnie połączyć.

![Interakcja z LLM jest obecnie bezstanowa, więc zawsze zaczyna się od nowa, o ile programistycznie nie zostaną wczytane wcześniejsze interakcje](https://cloud.overment.com/2024-08-05/aidevs3_stateless-2d7d20d2-f.png)

Idąc dalej, każdy model posiada kilka kluczowych cech, na które należy zwracać uwagę w pierwszej kolejności. Są to:

- **Wsparcie języków:** Większość modeli SOTA wspiera popularne języki, jednak zwykle są zoptymalizowane pod kątem języka angielskiego. O różnicach w samej skuteczności modeli można przeczytać w [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf) na stronie ósmej
- **Multimodalność:** Nie wszystkie modele wspierają przetwarzanie innych formatów niż tekst, ale zmierzamy w kierunku w którym multimodalność staje się powszechna
- **Format API:** Większość modeli posługuje się formatem ChatML lub jego zbliżoną alternatywą. Dostępne są także dodatkowe opcje związane z przetwarzaniem obrazu czy [strukturyzowaniem odpowiedzi](https://openai.com/index/introducing-structured-outputs-in-the-api/) 
- Context Window: Modele mogą przetwarzać określoną liczbę tokenów w ramach pojedynczego zapytania. Na ten limit składają się zarówno tokeny wejściowe (input) jak i tokeny wyjściowe (output)
- **Max Output:** Limit okna tokenów nie jest jedynym, ponieważ **LLM posiadają ograniczoną długość wypowiedzi** co utrudnia przetwarzanie długich treści
- **Knowledge Cutoff:** LLM są obecnie "zamrażane" po zakończeniu treningu, a ich wiedza obejmuje dane treningowe z konkretnego okresu. Zazwyczaj zależy nam, aby bazowa wiedza modelu była jak najbardziej aktualna, nie tylko ze względu na same informacje, lecz zdolność rozpoznawania słów kluczowych (np. nazw własnych)
- **Cena:** Rozliczenia niemal zawsze zależą od liczby przetworzonych tokenów i różnią się dla tokenów wejściowych oraz wyjściowych, a także formatu przetwarzanej treści. 

Z czasem powyższe cechy stają się mniejszym problemem wraz z rozwojem modeli. Trzeba jednak je mieć na uwadze, ponieważ w przypadku programistycznych zastosowań skala ma znaczenie. Nawet niska cena jednostkowa za tokeny może przekładać się na istotne kwoty w aplikacji produkcyjnej. Z kolei duże okno kontekstu może sugerować możliwość jednorazowego przetworzenia dużej ilości danych, ale w zamian negatywnie przekłada się na czas reakcji oraz koszty.

Zatem fundament bezpośredniej interakcji z LLM polega na tym, aby programistycznie dostarczyć ustrukturyzowany kontekst, który model przetwarza każdorazowo, w celu wygenerowania odpowiedzi zgodnej z naszymi założeniami (komentarz: pojawiają się jednak [możliwości cache'owania promptu w celu optymalizacji tego procesu](https://www.anthropic.com/news/prompt-caching)). 

## Sterowanie zachowaniem modeli

Praca z modelami charakteryzuje się tym, że pracując z nimi poruszamy się bardziej w obszarze prawdopodobieństwa i szansy uzyskania właściwej odpowiedzi, niż pewności jej otrzymania. Rodzi to naturalne problemy związane ze stabilnością aplikacji w logice których pojawia się LLM oraz (uzasadnione) wątpliwości dotyczące bezpieczeństwa. Sytuację dodatkowo komplikuje fakt, że nie posiadamy pełnej kontroli nad zachowaniem modelu, lecz możemy nim jedynie sterować, co może zarówno pomóc, jak i zaszkodzić.

<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1002823053?h=b4ed4229a8&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="00_playground"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

W lekcji S00E01 powiedziałem, że LLM nie mogą zmodyfikować wybranego już tokenu. Obrazuje to poniższy przykład, w którym model ma wygenerować odpowiedź na pytanie na podstawie posiadanej informacji. Pomimo że zadanie to jest proste, rezultat jest błędny, ponieważ celowo dodałem fragment, który rozpoczynał wypowiedź modelu, której ten nie był już w stanie naprawić. Inaczej mówiąc **negatywnie wpłynąłem na wypowiedź** wykorzystując swoją wiedzę na temat działania modelu.

![LLM nie może cofnąć aktualnie wygenerowanego tokenu](https://cloud.overment.com/2024-08-05/aidevs_konfabulation-733949c6-6.png)

W OpenAI Playground możemy skorzystać z trybu Completion, a w nim z opcji "Show Probabilities", która wyróżnia poszczególne tokeny, wyświetlając te brane pod uwagę podczas inferencji. Daje nam to niekompletny obraz tego, jak LLM generuje treści. Jednocześnie jasno z niego wynika, że kolejne tokeny są uzależnione zarówno od mojej wypowiedzi (promptu), ale też kolejnych tokenów generowanych przez model.

![LLM dobiera kolejne prawdopodobne tokeny określając prawdopodobieństwo ich wystąpienia, uwzględniając przy tym różne czynniki](https://cloud.overment.com/2024-08-05/aidevs3_generation-e09fb94b-b.png)

Znacznie więcej informacji na temat generowania treści daje nam narzędzie Gemma Scope z którego możemy dowiedzieć się, że LLM aktywuje swoje różne obszary w zależności od aktualnie przetwarzanych danych. 

![Gemma Scope pozwala na wgląd w sposób generowania treści przez LLM](https://cloud.overment.com/2024-08-05/aidevs_scope-5be72493-4.png)

Przydatne informacje można znaleźć także w publikacji Anthropic Research "[Mapping the Mind of a Large Language Model](https://www.anthropic.com/news/mapping-mind-language-model)". Opisuje ona, jak przetwarzana treść aktywuje różne "funkcjonalności" modelu związane z określonymi koncepcjami, co wpływa na sposób generowania odpowiedzi. 

![Wpływ przetwarzanej treści (Golden Gate Bridge) na aktywację różnych obszarów modelu Claude 3](https://cloud.overment.com/2024-08-08/aidevs3_map-34b0abb5-b.png)

Podobną ideę opisał David Shapiro w repozytorium "[Latent Space Activation](https://github.com/daveshap/latent_space_activation)", gdzie mówił o tym, że różne techniki projektowania promptów mają wspólną cechę, polegającą na tym, aby aktywować różne obszary modelu.

Powyższe informacje sugerują nam zatem, że naszym celem jest sterowanie zachowaniem modelu w celu zwiększania prawdopodobieństwa uzyskania pożądanego rezultatu, poprzez aktywację umiejętności modelu związanych z konkretnym zadaniem. Należy mieć na uwadze także fakt, że na zachowanie modelu wpływa cała przetwarzana treść, łącznie z tokenami, które są generowane.
## Struktura Promptu i formatowanie

Tworzenie instrukcji, czyli Prompt Engineering, jest bardzo otwartym zagadnieniem. Choć mamy do dyspozycji różne techniki projektowania, to trudno jednoznacznie stwierdzić, które z nich są słuszne oraz jak wiele z nich pozostaje nieodkryte. 

Skupmy się więc, na ogólnych zasadach, które są obecnie stosowane przy projektowaniu promptu, pamiętając o zachowaniu otwartości na nowe taktyki i dopasowanie schematów do własnych potrzeb.

Ogólna struktura promptu systemowego uwzględnia następujące elementy:

1. **Zawężenie kontekstu**: zwykle polega na nadaniu roli, opisie sytuacji czy problemu w celu uniknięcia dwuznaczności oraz nakierowania model na dany schemat myślenia. Np. "Jesteś programistą JavaScript" jasno sugeruje język w którym powinny być napisane przykłady kodu.
2. **Określenie celu**: może uwzględniać zarówno precyzyjny rezultat, lub ogólny kierunek interakcji z modelem. Np. "wypisz listę słów kluczowych w formacie JSON, zgodnie z poniższymi zasadami, strukturą oraz przykładami."
3. **Określenie zasad realizacji**: zazwyczaj jest to forma listy (większa czytelność) opisująca zachowanie modelu oraz sposób wykonania zadania. Zasady powinny obejmować zarówno zachowanie w idealnych warunkach, jak i sytuacje brzegowe (np. niewystarczające dane do realizacji celu)
4. **Dostarczenie informacji** (RAG): niemal zawsze będzie nam zależało na tym, aby w prompcie systemowym znalazła się sekcja zawierająca dodatkową wiedzę dla modelu (np. wyniki wyszukiwania). Sekcja ta powinna być bardzo wyraźnie oddzielona od pozostałych tak, aby model nie pomylił jej zawartości z instrukcjami, które ma realizować.
5. **Zaprezentowanie przykładów** (Few-shot Prompting): LLM świetnie rozpoznają wzorce, więc warto poświęcić czas, aby zaprezentować oczekiwane zachowanie na kilku zróżnicowanych przykładach, uwzględniających także warunki brzegowe czy niepożądane scenariusze, które model powinien zaadresować według naszych zasad.

Poniżej znajduje się przykładowy prompt, podzielony na sekcje (w tym przypadku pominąłem dostarczanie zewnętrznych informacji), którego celem jest wygenerowanie odpowiedzi "tak" lub "nie". Jako separatory poszczególnych sekcji, zastosowałem składnię przypominającą XML. W różnych źródłach można spotkać także separatory takie jak "###" czy "\`\`\`", jednak sam z nich już nie korzystam ze względu na konflikt w jaki wchodzą w składni Markdown, którą domyślnie stosuję. 

![Przykład prostego promptu, którego rezultatem jest odpowiedź tak lub nie](https://cloud.overment.com/aidevs3_sample-1723130499.png)

Działanie powyższego promptu można przetestować w OpenAI Playground pod tym adresem: [Prompt Yes / No](https://platform.openai.com/playground/p/8UA2vqhnxF14S3StYn2UqU9M?model=undefined&mode=chat). Jego celem jest jedynie zaprezentowanie ogólnej struktury i dość łatwo nadpisać jego zachowanie, co stanowi problem, który będziemy rozwiązywać na innych promptach. 

Strukturę promptu prezentuje poniższy schemat, w którym widoczna jest instrukcja systemowa i wiadomość użytkownika (których treść stanowi dane wejściowe / input tokens) oraz wiadomość asystenta (których treść stanowi dane wyjściowe / output tokens). Całość nie może przekraczać dopuszczalnej liczby tokenów dla danego modelu, czyli wspominany już Context Window. 

![Ogólna struktura promptu, input tokens, output tokens i context window limit](https://cloud.overment.com/2024-08-08/aidevs3_structure-f108237d-6.png)

Pisząc prompt, można zadać sobie następujące pytania: 

- Jaki jest jego cel?
- Jakie zadanie ma wykonać model, aby ten cel zrealizować?
- Jaki jest kontekst realizowanego zadania?
- Jakie ograniczenia chcę narzucić na model?
- Jaki format odpowiedzi chcę uzyskać?
- Co powinny prezentować przykłady, aby zobrazować to, o co mi chodzi?
- Czy prompt powinien zawierać jakieś nietypowe sekcje?
- Jak będzie wykorzystana odpowiedź wygenerowana przez model? Czy trafi do człowieka, czy zostanie zwrócona przekazana do kolejnego promptu?

W ten sposób możemy naszkicować pierwszą wersję promptu, a następnie testować go i wprowadzać kolejne usprawnienia, aż uzyskamy pożądane rezultaty. Wówczas, zamiast kończyć pracę, możemy zastanowić się także nad potencjalną optymalizacją (zmniejszenie objętości i/lub zwiększenie precyzji).

## Prompt Design i Prompt Engineering

Określenia "Prompt Design" i "Prompt Engineering" są stosowane zamiennie, lub używane jest wyłącznie to drugie. Taki podział pozwala na odróżnienie procesu tworzenia samej instrukcji od sposobu jej zastosowania. 

Mówiąc konkretnie:

- Prompt Design polega na opracowaniu instrukcji, która steruje zachowaniem modelu w określonym celu. W tym procesie istotny jest odpowiedni dobór słów, struktury, formatowania czy przykładów. 
- Prompt Engineering obejmuje stosowanie promptów w logice aplikacji bądź automatyzacji, jego ewaluację, optymalizację oraz kompozycję. Proces ten może obejmować łączenie promptów ze sobą oraz z kodem.

Choć taki podział nie jest oficjalny i może nawet zostać uznany za błędny, tak w praktyce daje dobrą perspektywę na temat umiejętności wymaganych na różnych etapach pracy z LLM. Podobnie przydatną perspektywą jest określenie "Flow Engineering", które bardziej nawiązuje do łączenia ze sobą wielu promptów.

![...](https://cloud.overment.com/2024-08-09/aidevs3_flow-117a505a-1.png)

## Najnowsze techniki projektowania promptów

Projektowanie promptów można sprowadzić do: 

- intuicyjnej aktywacji obszarów LLM mogących pomóc w rozwiązaniu danego problemu poprzez słowa kluczowe i/lub zewnętrzne dane. Choć obszary te nie są nam znane, to możemy intuicyjnie założyć, że np. przywołanie "modeli mentalnych" może ukierunkować model na określony sposób rozwiązania problemu
- zaprezentowania przykładów zawierających wzorce pożądanych zachowań (mówi się także o przykładach prezentujących niepożądane zachowania w celu wprowadzenia kontrastu. Nawiązuje do tego "[Large Language Models are Contrastive Reasoners](https://arxiv.org/abs/2403.08211)")
- sterowania zachowaniem modelu poprzez treści dostarczone przez nas, programistyczne funkcje lub sam model (lub modele), które dążą do zwiększenia prawdopodobieństwa na uzyskanie poprawnej odpowiedzi

Aktualnie dostępne modele językowe mogą skutecznie pomagać przy projektowaniu promptów, o czym możemy przeczytać w [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910). Dodatkowo, możemy je wspomagać wiedzą na temat technik opisanych w [Prompting Guide](https://www.promptingguide.ai/), w publikacjach wymienionych w [Goal-oriented Prompt Engineering](https://github.com/Alex-HaochenLi/Goal-oriented-Prompt-Engineering?tab=readme-ov-file), a także wykorzystać przydatne frazy lub przykłady promptów ze źródeł takich jak [repozytorium Fabric](https://github.com/danielmiessler/fabric/tree/main/patterns). To wszystko w połączeniu z opisem własnych preferencji czy kontekstu, pozwala na skuteczne generowanie szkiców promptów, które możemy dalej rozwijać i optymalizować.

Aby LLM mógł nam pomagać w pisaniu promptów, musimy opracować instrukcję, którą możemy określić mianem "meta-promptu". Przykład jednego z nich dostępny jest tutaj: [Snippet Prompt Generator](https://platform.openai.com/playground/p/JagfTtnjCim1u918mhSvnkfU?model=undefined&mode=chat). Wystarczy w pierwszej wiadomości opisać prompt, który chcemy stworzyć, a model przeprowadzi nas przez proces myślowy i ostatecznie naszkicuje pierwszą wersję instrukcji.

![Fragment meta-promptu, którego celem jest szkicowanie nowych promptów](https://cloud.overment.com/2024-08-09/aidevs3_metaprompt-38cb25d7-d.png)

Powyższy "meta-prompt" zawiera szereg reguł, słów kluczowych oraz opisów technik, które częściowo napisałem samodzielnie, a część wygenerowałem na podstawie analizy repozytorium [Fabric](https://github.com/danielmiessler/fabric) wspomnianego wcześniej.

Wniosek jest więc następujący: **Na każdym etapie procesu budowania instrukcji LLM może nas WSPIERAĆ w doborze słów kluczowych, parafrazy, przywołania definicji czy wygenerowania przykładów**. Taką współpracę można uznać jako fundamentalny element procesu projektowania promptów.

Prompt Engineering nie opiera się wyłącznie na jednym schemacie, choć możemy wyróżnić następujące kategorie, które można stosować w różnych konfiguracjach, w tym także łączyć je ze sobą:

- Zero-Shot: To prompt zawierający wyłącznie instrukcję, bez przykładów prezentujących oczekiwane zachowanie.
- Few-Shot: To prompt, który poza instrukcją zawiera kilka przykładów w których LLM może zauważyć schematy i wzorce. Istnieje także wariant "Many-Shot" w przypadku którego liczba przykładów może sięgać nawet kilkuset.
- Thought Generation: Czyli ciąg myśli, który może zostać dostarczony przez nas (few-shot Chain of Thought) lub zostać wygenerowany przez model (zero-shot CoT). Taki prompt prowadzi model przez pewien schemat myślowy, zwiększając skuteczność wypowiedzi. 
- Ensembling: Polega na wykorzystaniu wielu promptów lub tego samego promptu wiele razy, aby następnie porównać rezultaty i zdecydować o najbardziej prawdopodobnym wyniku.
- Self-Reflection: Polega na stworzeniu przestrzeni dla modelu na wygenerowanie refleksji na temat realizowanego zadania, które poprzedza ostateczną wypowiedź.
- Self-Criticism: Jak sugeruje nazwa, to technika polegająca na zastosowaniu LLM w celu krytyki swoich wcześniejszych wypowiedzi. W marcu 2024 roku została podważona w publikacji [Large Language Models Cannot Self-Correct Reasoning Yet](https://arxiv.org/abs/2310.01798), która jednocześnie sugeruje, że taka zdolność może się rozwijać w modelach kolejnej generacji.
- Decomposition: To technika opierająca się o rozbijanie trudnych zadań na mniejsze kroki, realizowane zwykle przez oddzielne prompty.

Wymienione wyżej kategorie (poza Self-Reflection) pochodzą z publikacji 
[A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608).

![Techniki projektowania promptów z podziałem na kategorie zero-shot, few-shot, thought generation, ensembling, self-criticism oraz decomposition](https://cloud.overment.com/2024-08-10/aidevs3_engineering-b1908395-0.png)

Większość z wymienionych technik wymaga czasochłonnego opracowania instrukcji, przykładów i nierzadko wymaga także uruchomienia wielu zapytań. Dlatego stosuje się je w połączeniu z logiką aplikacji lub automatyzacji, gdzie tworzymy wspomniany już Chain akcji wykonywanych w ustalonej kolejności lub Agent, gdzie kolejność działań ustalana jest dynamicznie.
## Prompty modularne i ich kompozycja

Prompty działające w logice aplikacji realizują określone cele na podstawie informacji, które mogą być współdzielone między nimi. Niekiedy będzie to wspólna sekcja (np. zestaw zasad opisujących sposób wypowiedzi modelu), a innym razem sekcja zawierająca rezultat innych promptów czy działania zewnętrznych narzędzi. 

Przykładem może być mapa wspomnień widoczna poniżej. W tym przypadku ma ona statyczną formę, jednak takie sekcje mogą być generowane na podstawie informacji z bazy danych bądź zewnętrznych źródeł. 

![Przykład sekcji opisującej uproszczoną mapę pamięci długoterminowej dla asystenta AI, która występuje w wielu promptach jako jedna z sekcji](https://cloud.overment.com/2024-08-10/aidevs3_memory-e44f3457-a.png)

Powyższa mapa może być zastosowana jako fragment promptów, które mogą jej potrzebować. Choć wydaje się to oczywiste, tak w praktyce przestaje takie być, ponieważ prompt w kodzie aplikacji szybko zaczyna wyglądać tak:

![Fragment modularnego promptu, zawierający dynamiczne sekcje](https://cloud.overment.com/2024-08-10/aidevs3_composition-0c43daae-7.png)

Szczególnie duży nacisk powinien więc być położony tutaj na wcześniejsze zaplanowanie ogólnej struktury promptu, separatorów sekcji oraz sposobu ich zapisu. Wymagana jest także precyzja oraz staranność, obejmująca chociażby obsłużenie sytuacji w której zawartość danej sekcji jest niedostępna.

Sekcje promptu pełnią różne cele, które nie powinny się wykluczać oraz powinny być od siebie bardzo wyraźnie oddzielone. Trzeba także zadbać o to, aby ograniczyć występowanie sekcji, które nie maja znaczenia w danej sytuacji. Choć obecnie LLM są w stanie przetwarzać duże zestawy danych, tak ich zdolność do podążania za bardzo złożonymi instrukcjami jest ograniczona.

Choć projektowanie modularnych promptów będzie różniło się w zależności od sytuacji, warto rozważyć kilka kwestii:

- Jak wygląda ogólna struktura promptu przed podziałem na sekcje?
- Które z sekcji występują wielokrotnie lub muszą być wczytane dynamicznie?
- Jaki rodzaj separatorów powinien być zastosowany, aby nie wejść w konflikt w samą treścią sekcji (np. stosowanie separatora "###" wchodzi w konflikt ze składnią markdown)
- Jaki rodzaj formatowania powinien być zastosowany w treści sekcji. Np. listy, zagnieżdżone listy, składnia markdown.
- W jakiej kolejności powinny zostać ułożone sekcje i czy ma to znaczenie z punktu widzenia modelu o czym mówi publikacja [Premise Order Matters in Reasoning](https://arxiv.org/abs/2402.08939). Choć prawdopodobnie będzie się to zmieniać w przyszłości, nadal można uznać, że LLM zachowuje najwięcej uwagi dla treści znajdujących się na początku lub na końcu instrukcji
- Jak zostaną dostarczone przykłady few-shot oraz czy muszą być dobierane dynamicznie, czy nie.
- Jaką rolę w logice aplikacji pełni dany prompt? Czy zależy od wyniku działania innych promptów, czy może być uruchomiony niezależnie (równolegle), oraz w jaki sposób będzie wykorzystany jego wynik (czy musi być odpowiednio formatowany)

W kompozycji promptów składających się z wielu komponentów kluczową rolę odgrywa ich monitorowanie z możliwością edycji oraz debugowania. Do takich zadań zwykle będziemy wykorzystywać narzędzia takie jak LangFuse lub własne implementacje.
## Podstawy ewaluacji

Wprowadzając zmiany w kodzie, jesteśmy w stanie precyzyjnie określić ich konsekwencje. Pomimo tego i tak łatwo jest popełnić błędy, które można wykryć poprzez testy manualne i automatyczne. W przypadku promptów jest podobnie, jednak sytuacja jest nieco bardziej złożona, ponieważ dokładnie nie wiemy jak nawet najmniejsze zmiany przełożą się na działanie modelu. Dlatego krytyczna staje się tu Evaluation, którą będziemy zajmować się w dalszej części AI_devs 3.

Poniżej widoczny jest fragment testu wykonanego z pomocą narzędzia PromptFoo, które aktualnie jest jednym z najbardziej użytecznych rozwiązań w swojej kategorii. Dzięki niemu możemy zdefiniować testy, zestawy danych oraz prompty oraz sprawdzić ich działanie na różnych modelach. 

![Przykład automatycznej weryfikacji działania promptu klasyfikującego zapytanie pod kątem użycia wyszukiwarki internetowej](https://cloud.overment.com/2024-08-10/aidevs3_evaluation-8ff19513-b.png)

Dane testowe możemy generować przy współpracy z modelem, jednak najwiecej wartości dadzą nam faktyczne zapytania pochodzące z produkcyjnie działającej aplikacji. [Instalacja PrompFoo](https://www.promptfoo.dev/docs/installation/) sprowadza się do jednego polecenia opisanego w dokumentacji, np. `npm i -g promptfoo`. Następnie w katalogu, w którym chcemy utworzyć testy, wykonujemy polecenie `promptfoo init`, które wygeneruje podstawowy plik YAML, oraz `promptfoo eval`, aby uruchomić test. 

Testy mogą być deterministyczne, gdzie zależy nam na sprawdzeniu konkretnych wartości, weryfikowane programistycznie (np. z pomocą wyrażeń regularnych) lub mogą być oceniane z pomocą modelu (czyli dodatkowych promptów). Choć ewaluacją będziemy zajmować się w lekcji C01L04, to już teraz warto przynajmniej uruchomić PromptFoo i wykonać kilka prostych testów. 
## Projektowanie przykładów "few-shot"

Odpowiednio dobrane przykłady dołączane do promptu, niemal zawsze mają pozytywne przełożenie na działanie modelu. Jednak ich obecność może także mieć negatywny wpływ, ponieważ LLM generując treści może sugerować się nimi zbyt mocno.

Choć realizowane zadanie można dokładnie opisać, tak trudno jest słowami oddać subtelne detale czy zasady na których nam zależy. Dlatego jako część instrukcji wystarczy dodać sekcję zawierającą (zwykle) pary w postaci wiadomości użytkownika oraz odpowiedzi ze strony AI. Taka sekcja musi być wyraźnie oddzielona od reszty instrukcji.

![Przykłady zwykle pojawiają się po głównej instrukcji, stanowiąc jej rozszerzenie](https://cloud.overment.com/2024-08-10/aidevs3_examples-1b607ecb-0.png)

Stworzenie zestawu jakościowych przykładów jest żmudne i wymagające. W dodatku nie możemy jednoznacznie stwierdzić jak wiele z nich potrzebujemy oraz który wspiera nasz cel, a który wprost przciwnie.

Tworząc przykłady, można rozważyć następujące aspekty:

- Przykład typowego zapytania użytkownika i typowej odpowiedzi, które zobrazują ogólne założenia.
- Przykład sytuacji w której zapytanie użytkownika jest niezwiązane z zakresem danej instrukcji (można to porównać do bloku `else` w instrukcjach warunkowych)
- Przykłady wzmacniające ważne elementy instrukcji (np. unikanie wybranych zwrotów czy zachowanie kolejności wypowiedzi)
- Przykłady prezentujące sytuacje w których nasza instrukcja może być zrozumiana niejednoznacznie (np. klasyfikacja wpisu to-do może sugerować inną kategorię niż ta, na której nam zależy)
- Przykłady prezentujące ton, pisownię oraz format wypowiedzi modelu
- Przykłady prezentujące sposób przetwarzania treści, które wykraczają poza bazową wiedzę modelu (np. nazwy własne) czy domyślne zachowanie modelu
- Przykłady nie powinny prezentować zbyt wielu podobnych sytuacji, aby nie skrzywić zachowania modelu i tym samym nie wpływać negatywnie na generowane odpowiedzi

Few-Shot prompting nie jest jednak rozwiązaniem ostatecznym. Z jego pomocą nie sprawimy, że model będzie w stanie rozwiązywać złożone zadania. Poza tym, w przypadku promptów działających w konwersacji (np. chatboty), wraz z rozwojem dyskusji zauważalne jest pomijanie niektórych zasad i odchodzenie od zachowań zaprezentowanych na przykładach. Nie stanowi to jednak zwykle dużego problemu, ponieważ logika którą będziemy tworzyć będzie składać się z wielu promptów oraz na różne sposoby kompresować treść konwersacji. 

Pracę nad zestawami przykładów możemy bardzo ułatwić sobie z pomocą LLM. Nie jest to jednak zadanie, które model może zrealizować samodzielnie i nie powinniśmy wyłączać swojej uwagi. Każdy z wygenerowanych przykładów powinien być przez nas przeczytany oraz przemyślany. 

Poniżej znajduje się fragment konwersacji z przykładami wygenerowanymi przez model. Przynajmniej dwa z nich nie były zgodne z moimi założeniami, w tym jeden z nich w ogóle nie powinien pojawić się w prompcie. 

![Przykłady promptu generowane przez LLM](https://cloud.overment.com/2024-08-10/aidevs3_generated_examples-16de4d0e-2.png)

W rezultacie nie wykorzystałem praktycznie żadnego z początkowo wygenerowanych przykładów, lecz na ich podstawie doszedłem do nowego, dopasowanego do mnie zestawu.

![](https://cloud.overment.com/2024-08-10/aidevs_used_examples-e1097244-c.png)

Powyższa sytuacja jest prawdopodobnie najważniejszą lekcją z całego AI_devs 3. Mówi ona o tym, aby nie wyłączać swojego rozumu podczas pracy z modelem. Zamiast zastępować swoje umiejętności, możemy je rozwijać, wspierać i optymalizować. Możliwe, że z czasem granica odpowiedzialności oraz kompetencji będzie się coraz bardziej przesuwać, jednak obecnie podejście, w którym to my kontrolujemy proces, generuje lepsze rezultaty.
## Zwiększanie precyzji

Niedeterministyczna natura LLM sprawia, że nie mamy gwarancji otrzymania pożądanego rezultatu nawet wtedy, gdy dane wejściowe pozostaną takie same. Możemy jednak zwiększać prawdopodobieństwo ich otrzymania. Z drugiej strony nie mamy do dyspozycji narzędzi w stylu `debuggera` kodu, co utrudnia wprowadzanie poprawek. Możemy jednak obserwować zachowanie modelu dzięki LangFuse lub nawet prostemu systemowi logów stworzonemu na własne potrzeby i **obserwacja jest pierwszą rzeczą, którą możemy zrobić**.

Poniższy prompt na pierwszy rzut oka realizuje swoje zadanie poprawnie i tłumaczy przekazany przez użytkownika tekst na język polski. Taka instrukcja może być zastosowana w narzędziach ułatwiających edycję dokumentów lub bezpośrednio w logice przetwarzającej je automatycznie. 

![Przykład prostego promptu, który tłumaczy przekazany tekst na język polski](https://cloud.overment.com/2024-08-11/aidevs3_translation-2fbc49b1-6.png)

Może się więc zdarzyć, że przetwarzany fragment będzie wyglądał tak, jak gdyby był instrukcją dla modelu. Wówczas zamiast otrzymać odpowiedź zgodną z początkowym założeniem, otrzymujemy zupełnie inny rezultat. 

![Zachowanie modelu można łatwo nadpisać, odwracając jego uwagę od domyślnego zachowania](https://cloud.overment.com/2024-08-11/aidevs3_wrong_translation-bbbd3837-e.png)

Mamy więc do czynienia z sytuacją, w której prompt nie jest precyzyjny i bardzo łatwo wygeneruje problemy. Widzimy jednak, że ich źródłem jest fakt, że model podąża za instrukcjami użytkownika, więc musimy zmodyfikować prompt, aby zmniejszy ryzyko takiego zachowania.

Nowa wersja promptu mówi więc o tym, aby model ignorował treść otrzymanej wiadomości i skupił się wyłącznie na przetłumaczeniu jej zawartości. Nie jest to jednak wystarczające.

![Modyfikacja promptu zmienia zachowanie modelu, ale rezultat dalej nie jest zgodny z oczekiwaniem, ponieważ LLM dalej podążą za poleceniami użytkownika.](https://cloud.overment.com/2024-08-11/aidevs3_improvement-a8b35f19-e.png)

Na tym etapie wiemy już, że technika Few-Shot może nam w takiej sytuacji pomóc. Zatem ponownie zmieniam treść instrukcji sugerując podążanie za zaprezentowanymi przykładami oraz dodaję sekcję, która je zawiera. Co więcej, wybrałem je w taki sposób, aby oddać problem z którym do tej pory się mierzyłem. 

Jak widać poniżej, wprowadzone zmiany przyniosły efekt i tym razem tłumaczenie jest poprawne. Oczywiście nie oznacza to, że zaadresowaliśmy w ten sposób wszystkie możliwe problemy, lecz widoczny jest schemat, który możemy stosować. Połączenie tego z narzędziami do monitorowania i automatycznego testowania promptów, pozwala uzyskać zadowalający poziom skuteczności.

![Podanie przykładów pomaga w sterowaniu zachowaniem modelu](https://cloud.overment.com/2024-08-11/aidevs3_fix-0d583298-b.png)

Może się także okazać, że podejmowane próby naprawienia błędnie działającego promptu zawiodą. Wówczas konieczna jest zmiana strategii lub całkowite wycofanie się z zastosowania LLM w danym przypadku.
## Techniki anonimizacji i bezpieczeństwo przetwarzania danych

Wszystkie informacje, które trafiają do interakcji z modelem, powinny być uznawane za dostępne dla użytkownika, nawet w przypadku Open Source LLM czy hostowania modeli z pomocą usług takich jak Azure OpenAI Service. Oznacza to dokładnie, że:

- Większość dostępnych usług generative AI dąży do zachowania danych w celu dalszego trenowania modeli. Należy więc zwracać uwagę na polityki prywatności oraz zachować ograniczone zaufanie wobec nich
- Każdy prompt systemowy można przechwycić technikami Prompt Injection
- Zachowanie modelu można w dowolny sposób nadpisać poprzez Jailbreaking
- Przechowywanie jakichkolwiek niepublicznych danych w treści wiadomości systemowych stanowi wyzwanie z punktu widzenia bezpieczeństwa
- Akcje w przypadku których LLM ma prawo zapisu lub modyfikacji danych powinny być łatwo odwracalne oraz nie mogą obejmować akcji krytycznych ze względu na możliwość uruchomienia ich przypadkowo lub przez osoby trzecie
- Aktualną skuteczność samych modeli można określić (w zależności od implementacji) na około 70-95%, co jest niewystarczające dla sytuacji w których potrzebujemy pewności
- Każde działanie modelu powinno być nadzorowane przez człowieka oraz monitorowane
- Aplikacje wykorzystujące duże modele językowe powinny narzucać na nie programistyczne ograniczenia oraz możliwie ograniczać kontakt z użytkownikiem
- Skuteczność technik i narzędzi określanych jako Guardrails nadal jest niewystarczająca, co doskonale obrazuje na przykładzie profil [Pliny The Prompter](https://x.com/elder_plinius), który w dniu pisania tych słów zdołał złamać każdy z popularnych systemów i modeli językowych

Poniżej mamy przykład prostej sytuacji obrazującej problem. Sekretne hasło zostało zapisane w wiadomości systemowej, a więc zwykle nie będzie ono widoczne dla użytkownika nawet jeśli o nie bezpośrednio zapyta. 

![Domyślnie LLM nie ujawnia użytkownikowi informacji zawartych w polu "System"](https://cloud.overment.com/2024-08-05/aidevs3_secret-6bf2ce63-b.png)

Wystarczy jednak bardzo prosta technika polegająca na niebezpośrednim poprowadzeniu modelu do ujawnienia hasła. Instrukcja polega na tym, aby najpierw model potwierdził, że nie może czegoś zrobić, a po separatorze zapisał coś dokładnie odwrotnego. Jak widać, wykonał prośbę bardzo precyzyjnie, ale zdradził frazę, której nie powinien podawać.

![Możliwe jest zastosowanie prompt injection w celu uzyskania dostępu do pozornie niedostępnych informacji](https://cloud.overment.com/2024-08-05/aidevs3_injection-c2f6a104-8.png)

Jak widać nawet w powyższym prompcie, techniki Prompt Injection zwykle polegają na znalezieniu sposobu umożliwiającego odwrócenie uwagi modelu od głównych założeń. Ze względu na to, że mamy tutaj do czynienia z językiem naturalnym, możliwych wariantów ataku jest bardzo dużo, wliczając w to "leetspeak" czy alfabet morsa. 

## Podsumowanie

Choć przez wiele osób Prompt Engineering jest lekceważony, tak w przypadku integracji LLM z kodem aplikacji, tworzenie precyzyjnych instrukcji dla modelu jest krytyczne. Tym bardziej, że dość rzadko mówimy tutaj o pisaniu promptu, a bardziej o jego kompozycji i budowaniu z dynamicznych treści. Obserwujemy także rozwój technik projektowania promptów, a także wzorców zaawansowanych interakcji z modelem, które również pomagają w budowaniu złożonych systemów.

Najważniejszym tematem tej lekcji jest budowanie promptu z pomocą kodu oraz programistyczna interakcja z modelem. Tematem uzupełniającym (ale równie ważnym) jest zastosowanie narzędzia PromptFoo (bądź alternatywy) w celu testowania zachowania modelu.

Jeśli chodzi o kluczowe koncepcje, to warto zapamiętać, że pracując z LLM mówimy o poruszaniu się w obszarze prawdopodobieństwa, a nie pewności. Musimy mieć to na uwadze już na wczesnym etapie projektowania założeń aplikacji, aby uniknąć korzystania z modelu do zadań, w których zwyczajnie się nie sprawdzi.