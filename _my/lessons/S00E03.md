![](https://cloud.overment.com/S00E03-2-1730634394.png)

# S00E03 — API

LLM API pozwala nie tylko na dostosowanie interakcji z modelem, ale także budowanie złożonej logiki dzięki której LLM może połączyć się z narzędziami czy pamięcią długoterminową. Taka interakcja nierzadko składa się z kilkunastu czy nawet kilkudziesięciu zapytań do modelu, co może przełożyć się także na znaczny [wzrost jakości generowanej treści](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/). 

Poniższy screen pochodzi ze strony DeepLearning.ai i przedstawia porównanie modeli GPT-3.5 oraz GPT-4 w połączeniu z technikami budowania interakcji będących częścią Agentic Workflow. 

![GPT-3.5 w połączeniu z technikami takimi jak 'Mulit Agent', 'Planning' czy 'Tool Use' może osiągać większą skuteczność niż GPT-4](https://cloud.overment.com/2024-08-11/aidevs3_agentic-8f8ad6a2-7.png)

To wszystko możemy połączyć z faktem, że dzięki API dane mogą być przetwarzane przez LLM bez aktywnego zaangażowania użytkownika w każdy z etapów. To otwiera możliwość tworzenia automatyzacji czy narzędzi zdolnych do zaawansowanego przetwarzania treści. 

## Parametry API

Część platform i narzędzi dostosowuje swoje API tak, aby było spójne z OpenAI, co w praktyce ułatwia przełączanie się pomiędzy nimi. Widoczne są jednak różnice we wsparciu funkcjonalności (np. JSON Mode) w przypadku usług takich jak Groq czy narzędzia ollama. Z kolei przykład Anthropic pokazuje bardzo duże rozbieżności, co utrudnia korzystanie z obu platform po stronie kodu.

Zastosowanie wielu modeli jest częstą praktyką ze względu na kwestię optymalizacji kosztów, wydajności, prywatności lub dopasowania do własnych potrzeb (Fine-Tuning). Poza tym, dobrze jest utrzymywać możliwość przełączenia się pomiędzy różnymi dostawcami ze względu na rozwój modeli lub samych usług.

Niezbędne parametry niemal każdego API to `model` oraz `messages`. Model wskazuje na nazwę LLM, który ma wygenerować odpowiedź. Z kolei lista wiadomości zawiera treść całej interakcji, dzieląc ją na role takie jak `system`, `user`, `assistant` oraz `tool`. Ze względu na rozwój modeli multimodalnych, treść wiadomości może być zarówno ciągiem znaków jak i tablicą zawierającą różne formaty treści, które omówimy w późniejszych lekcjach. 

Zatem interakcja z LLM przez API polega w uproszczeniu na budowaniu listy wiadomości, których całą treść każdorazowo przetwarza model. Oczywiście nic nie stoi na przeszkodzie, aby programistycznie na nią wpływać, zmieniając tym samym zachowanie modelu. Przykładem takiej modyfikacji może być unikanie dostarczania wszystkich wiadomości na rzecz podsumowania umieszczanego w wiadomości systemowej.

![Domyślne parametry API umożliwiające interakcję z LLM](https://cloud.overment.com/2024-08-12/aidevs3_default_params-ea902572-1.png)

Choć interakcja z modelem może odbywać się bezpośrednio poprzez zapytania HTTP, tak obecnie lepiej jest korzystać z oficjalnych SDK [OpenAI](https://platform.openai.com/docs/libraries) lub [Anthropic](https://docs.anthropic.com/en/api/client-sdks) . Interesującym rozwiązaniem jest także Vercel AI SDK (TypeScript) lub [LiteLLM](https://github.com/BerriAI/litellm) (Python) ze względu na wsparcie różnych platform.

Dodatkowe parametry API obejmują także:

`max_tokens` — określa maksymalną liczbę tokenów wygenerowanych przez model. Wartość ta, dodana do liczby tokenów w prompcie, nie może przekraczać dopuszczalnej wartości Context Window. Jeśli model posiada limit 4096 tokenów, a `max_tokens` ustawiony jest na 1000, to prompt może zawierać maksymalnie 3096 tokenów. W przeciwnym razie otrzymamy błąd.
`temperature` — określa się ją jako "poziom kreatywności" modelu. Jest to wskaźnik losowości, którego wartość ustawiona na `0` sprawia, że wybierane są tylko najbardziej prawdopodobne tokeny, a odpowiedź jest mniej zróżnicowana. Widać to na animacjach poniżej

![Wartość Temperature ustawiona na 0 sprawia, że dobierane są tylko najbardziej prawdopodobne tokeny](https://cloud.overment.com/aidevs3_temp0-1723561941.gif)

![Wartość Temperature ustawiona na 1 sprawia, że model może wybrać mniej prawdopodobne tokeny w swojej wypowiedzi](https://cloud.overment.com/aidevs3_temp1-1723561986.gif)

`top_p` — Działa podobnie jak `temperature`, lecz zamiast wpływać na losowość, **wpływa na ilość** rozpatrywanych tokenów. Jego wartość `0 - 1` odnosi się do sumy prawdopodobieństwa poszczególnych tokenów. Jeśli zatem w sytuacji z poniższego zrzutu ekranu `top_p` ustawilibyśmy na `0.5`, to tylko token `crisp — 54.20%` byłby brany pod uwagę, ponieważ on sam przekracza sumę prawdopodobieństwa określoną przez `top_p`

![Parametr top p pozwala na zawężenie zakresu tokenów, które mogą zostać wybrane podczas inferencji](https://cloud.overment.com/2024-08-13/aidevs3_topp-a34f9629-b.png)

`frequency_penalty` — obniża prawdopodobieństwo wystąpienia danego tokenu, poprzez nakładanie na niego kary uzależnionej od tego, ile razu został już wybrany w dotychczas generowanej treści. 
`presence_penalty` — podobnie jak `frequency_penalty` obniża prawdopodobieństwo wybrania tokenu, ale kara jest uzależniona od samej obecności tokenu, a nie tego, ile razy się pojawił.
`logit_bias` — umożliwia podanie ID tokenu oraz wartości z zakresu `-100 - 100`. Wartość `-100` wyklucza token, a `100` niemal wymusza jego użycie. Identyfikatory tokenów można znaleźć dzięki Tiktokenizer i będą się one różnić w zależności od tokenizera wykorzystywanego przez model.
`stop` — to lista maksymalnie czterech sekwencji, które zatrzymają dalsze generowanie wypowiedzi
`n` — określa liczbę generowanych odpowiedzi dla tego samego promptu. To użyteczny parametr w przypadku Self-Consistency (technika polegająca na wybieraniu najlepszego rezultatu spośród kilku wariantów)
`user` — to ID użytkownika z naszej bazy, które pozwoli na zidentyfikowanie zapytań pochodzących od niego. To jedno z zaleceń [dobrych praktyk na produkcji](https://platform.openai.com/docs/guides/production-best-practices), rekomendowanych przez OpenAI.

Poza nimi mamy także parametry związane ze strukturyzowaniem odpowiedzi (JSON) oraz wykorzystywaniem narzędzi (Tool), ale wrócimy do nich w dalszych lekcjach. Dodatkowo wymienione parametry te mogą różnić się w zależności od API.
## Bezpośrednie połączenie z API, SDK i frameworki

Decyzja o sposobie połączenia z modelem jest ważna z punktu widzenia elastyczności aplikacji. Przykładowo, w OpenAI istnieje `Assistant API`, ułatwiające pracę z dokumentami i narzędziami. Z kolei frameworki takie jak LangChain czy CrewAI oferują warstwę abstrakcji, pozwalającą na szybkie tworzenie projektów. Jednak każde z wymienionych narzędzi ma ograniczenia związane z dopasowaniem implementacji do indywidualnych potrzeb. Poza tym, ze względu na ich wczesny etap rozwoju, ich zastosowanie na produkcji jest problematyczne z powodu częstych zmian API i niepełnych dokumentacji.

Choć przełączanie się pomiędzy różnymi modelami czy nawet ich wersjami nie jest oczywiste, to warto na tym etapie zostawić sobie przestrzeń na łatwą zmianę usługi bądź połączenie innego API.

Wniosek jest zatem następujący:

- Frameworki warto mieć na uwadze i monitorować ich rozwój. W niektórych przypadkach mogą sprawdzić się w komponentach aplikacji i z czasem mogą stać się w pełni użyteczne
- Bezpośrednie połączenie z API daje najwięcej kontroli, ale też wymaga dodatkowego nakładu pracy. API popularnych usług oferujących dostęp do LLM nadal się rozwija i pojawiają się w nim nowe opcje, co utrudnia utrzymanie samodzielnej integracji
- SDK dla popularnych języków, rozwijane jest bezpośrednio przez firmy tworzące dane modele lub narzędzia. Nowe wersje zawierające wsparcie dla nowych funkcji pojawiają się szybko, a samo SDK zwykle nie jest zbyt skomplikowane, co pozwala skupić się na rozwoju własnej logiki wokół niego

Oczywiście są scenariusze, gdzie np. `Assistant API` może sprawdzić się lepiej niż własna integracja. Przykładem może być tworzenie PoC (Proof of Concept) lub MVP (Minimum Viable Product), które wymagają szybkich iteracji.

Prosty przykład połączenia z modelem poprzez SDK (w tym przypadku OpenAI) znajduje się w przykładzie [sdk](https://github.com/i-am-alice/3rd-devs/tree/main/streaming), po uruchomieniu którego włączony zostanie serwer Node.js do którego możemy kierować następujące zapytania:

> curl -X POST http://localhost:3000/api/chat \
-H "Content-Type: application/json" \
-d '{"messages":[{"role":"user","content":"Hello"}]}'

Przesłana lista wiadomości zostaje połączona z wiadomością systemową, a następnie przekazana do modelu. W rezultacie otrzymujemy odpowiedź zawierającą minimalną liczbę słów, ponieważ o tym właśnie mówi nasz główny prompt.

![](https://cloud.overment.com/2024-08-27/aidevs3_sdk-66fa8184-9.png)

## Połączenie z modelami Open Source

W lekcji S00E01 wspomniałem o ollama jako prostym sposobie na połączenie z modelami Open Source na własnym komputerze, aczkolwiek musi on spełniać minimalne wymagania, które głównie ukierunkowane są na pamięć RAM oraz GPU. Z modeli tych można także skorzystać poprzez API za pośrednictwem platform Groq, Replicate czy OpenRouter. Po ukończeniu AI_devs 3 warto zapoznać się także z [MLX](https://github.com/ml-explore/mlx), [llama.cpp](https://github.com/ggerganov/llama.cpp) i bezpośrednio biblioteką [Transformers](https://huggingface.co/docs/transformers/en/index) (ma także swój odpowiednik Transformers.js).

Choć może się wydawać, że korzystanie z Open Source LLM jest bezpłatne, to w praktyce koszty zakupu lub [wynajęcia sprzętu](https://www.runpod.io/pricing) oraz jego utrzymanie, osiągają podobny lub nawet większy poziom niż w przypadku modeli komercyjnych. Z drugiej strony mamy nad nimi pełną kontrolę, ale też musimy liczyć się z tym, że jakość generowanych odpowiedzi jest znacznie niższa (choć wystarczająca do wielu zadań, a same modele Open Source nieustannie się rozwijają).

Dodatkowo rozwijają się także modele określane jako SLM (Small Language Model), które w swoim założeniu mają działać na urządzeniu użytkownika (np. smartphone). Chociażby z tego powodu warto obserwować rozwój modeli z tej kategorii.

Dzięki ollama możemy sprawdzić działanie małego modelu na własnym komputerze, bez ponoszenia dodatkowych kosztów. Wystarczy, że [po instalacji ollama](https://ollama.com/) w terminalu uruchomimy polecenie `ollama run gemma2:2b`. Spowoduje to pobranie modelu na nasz komputer, a następnie możliwość poprowadzenia z nim rozmowy, co widać na poniższym zrzucie ekranu:

![Wymiana wiadomości z modelem działającym lokalnie na naszym komputerze](https://cloud.overment.com/2024-08-15/aidevs3_ollama-6a84ecf4-0.png)

Ollama umożliwia także znacznie wygodniejszą interakcję z modelem, poprzez zapytania HTTP na adres `localhost:11434/api/chat` w formacie zgodny z OpenAI API.

![Ollama umożliwia interakcję z modelem poprzez localhost, w formacie niemal w pełni zgodnym z OpenAI API](https://cloud.overment.com/2024-08-15/aidevs3_ollama_request-a3e14e8a-b.png)

**Ważne:** Pierwsze zapytanie do modelu Open Source trwa dłużej ze względu na konieczność wczytania go do pamięci. Kolejne interakcje są już znacznie szybsze, ale zależą od wykorzystywanego modelu oraz konfiguracji sprzętowej. 

Na temat modeli Open Source dostępnych na ollama należy wiedzieć przede wszystkim, że:

- Końcówki `:2b, :8b, :27b` itd. oznaczają liczbę parametrów modelu. Im więcej parametrów, tym większe wymagania sprzętowe, ale też większe możliwości samego modelu (choć nie zawsze).
- Modele mogą być aktualizowane przez ollama, co zwykle obejmuje poprawki błędów związanych z inferencją
- Modele zwykle poddawane są także kwantyzacji, czyli optymalizacji w celu przyspieszenia inferencji oraz zmniejszenia wymagań. Wersje `Q4_K_M`, `Q5_K_S` oraz `Q5_K_M` są uznawane jako rekomendowane. 
- Modele Open Source mogą zostać dopasowane do naszych potrzeb poprzez Fine-Tuning. Proces ten ułatwiają rozwiązania takie jak Unsloth
- Modele Open Source nierzadko wymagają większych umiejętności Prompt Engineeringu ze względu na bardziej ograniczoną zdolność do podążania za instrukcjami
- Modele na które warto zwrócić uwagę to `llama 3.1`, `gemma 2`, `qwen2`, `phi3`, `mistral`, `deepseek coder`, `llava` (vision), `MiniCPM` (vision) i `whisper` (speech to text), `ChatTTS` (text to speech)

W AI_devs 3 tylko mała część naszego czasu będzie poświęcona modelom Open Source, ponieważ projektowane przez nas interakcje będą wymagały realizowania wielu zapytań, często równolegle. Z drugiej strony techniki pracy z modelami komercyjnymi można później przełożyć na modele lokalne i ukierunkować dalszy rozwój właśnie na ten obszar.

Do pracy z modelami Open Source warto także skorzystać z graficznego interfejsu, np. [AnythingLLM](https://anythingllm.com/) lub [ChatbotUI](https://github.com/mckaywrigley/chatbot-ui)

## Podstawy prowadzenia interakcji z modelem

LLM w kodzie aplikacji pozwala nam na znacznie więcej niż w ChatGPT czy Claude.ai. Możemy swobodnie wpływać zarówno na przetwarzaną treść, ale też wykonywać serię różnych akcji, przed wygenerowaniem ostatecznej odpowiedzi. Z drugiej strony musimy samodzielnie zadbać o rzeczy, które wspomniane usługi oferują nam domyślnie. 

Wiemy już, że interakcja z obecnymi modelami odbywa się poprzez API i format ChatML. Zatem to do nas należy zbudowanie promptów, wczytanie dynamicznych danych, historii konwersacji oraz dostarczenie odpowiedzi użytkownikowi. Poniżej znajduje się schemat logiki, uwzględniającej:

- Wiadomość użytkownika, która początkowo trafia do dwóch oddzielnych promptów. 
- Pierwszy prompt generuje odpowiedź na podstawie informacji z bazy danych (np. pamięci długoterminowej i danych zapisanych na temat użytkownika)
- Drugi prompt generuje odpowiedź na podstawie wyników wyszukiwania z Internetu
- Pierwszy i drugi prompt wykonuje się **równolegle** w celu zmniejszenia czasu potrzebnego na wykonanie całego schematu
- Treść wygenerowana przez te prompty przekazywana jest do trzeciego, którego zadaniem jest wygenerowanie odpowiedzi przesyłanej do użytkownika
- W rezultacie taka logika pozwala na wygenerowanie spersonalizowanej odpowiedzi, uwzględniającej aktualne dane, mogące wykraczać poza bazową wiedzę modelu

![Przykład interakcji z LLM, w której logika uwzględnia dodatkowe kroki przez wygenerowaniem odpowiedzi](https://cloud.overment.com/2024-08-15/aidevs3_logic-d55f9c57-0.png)

Choć logika prezentowana przez powyższy schemat może sugerować wprost nieograniczone zastosowania LLM, to wiąże się z nią szereg wyzwań, które musimy zaadresować na poziomie aplikacji. Przykładowo sama konfabulacja stanowi tutaj jeszcze większy problem niż w ChatGPT, ponieważ jeśli pojawi się nawet na jednym z etapów, to może negatywnie wpłynąć na kolejne. To samo dotyczy się błędów, także tych w kodzie aplikacji, niemających nic wspólnego z modelem.

Do projektowania zaawansowanej logiki jeszcze wrócimy i na ten moment wystarczy świadomość tego, że projektowane interakcje będą znacznie wykraczać poza wymianę informacji bezpośrednio z modelem. 

W folderze `sdk` repozytorium AI_devs znajduje się przykład prostego serwera HTTP, którego zadaniem jest wygenerowanie odpowiedzi przez model OpenAI za pośrednictwem oficjalnego SDK dla Node.js. Interakcja jest dodatkowo wzbogacona o **systemowy prompt** sprawiający, że model będzie używał minimalnej liczby słów. Treść tej wiadomości jest niemożliwa do nadpisania przez użytkownika, ale wiemy z S00E02, że dzięki Prompt Injection może on uzyskać jej treść. 

![Przykład prostego endpoint'u umożliwiającego interakcję z LLM OpenAI, którego instrukcja systemowa ustawiana jest po stronie serwera](https://cloud.overment.com/2024-08-15/aidevs3_sdk-988e5c1d-5.png)

Testowe zapytanie potwierdza wpływ instrukcji systemowej na LLM. Teraz także jasne jest, że **kompletna lista wiadomości** musi zostać przesłana **w każdym zapytaniu**. W przeciwnym wypadku konwersacja wciąż będzie zaczynać się od nowa, a model nie będzie mieć wiedzy o poprzednich wiadomościach. 

![Zapytanie do przykładu 'sdk' z repozytorium pokazuje w jaki sposób możemy sterować zachowaniem LLM poprzez instrukcję systemową, na którą użytkownik nie ma wpływu](https://cloud.overment.com/2024-08-15/aidevs3_chat-7c67fee3-9.png)

Zatem jeśli chcielibyśmy kontynuować konwersację, to musimy zadbać o przekazanie zarówno wiadomości użytkownika, jak i dotychczasowych odpowiedzi LLM.

![Prowadzenie konwersacji z modelem wymaga każdorazowego przesyłania kompletnej listy konwersacji](https://cloud.overment.com/2024-08-15/aidevs3_convo-db9be435-2.png)

Naturalnie, nic nie stoi na przeszkodzie, aby zmienić powyższy schemat. Zamiast przesyłać całą listę wiadomości, moglibyśmy zapisać podsumowanie konwersacji w bazie danych, a następnie wczytać je do promptu systemowego. Choć mówimy tutaj o kompresji (i część danych zostanie pominięta), to pozwoli nam to zredukować koszty oraz uniknąć przekroczenia Context Window. Dobrze poprowadzone podsumowanie może zwiększyć skuteczność promptu, ponieważ uwaga modelu pozostanie skupiona na instrukcji systemowej.

![Rozmowa z LLM może opierać się o listę wiadomości lub podsumowanie dodane do instrukcji systemowej](https://cloud.overment.com/2024-08-15/aidevs3_compression-b513b064-0.png)

Wniosek jest zatem następujący: 

- Choć ChatML sugeruje, że wymiana wiadomości odbywa się pomiędzy użytkownikiem, a asystentem, to w praktyce nie musi tak być. Treść wiadomości użytkownika może być zamieniona dowolnymi innymi danymi wejściowymi
- Treść wiadomości może być modyfikowana i nadpisywana w trakcie interakcji z modelem
- Ze względu na Prompt Injection i Jailbreaking będzie nam zwykle zależało na ograniczeniu interakcji LLM z użytkownikiem
- Logika realizowana przez LLM musi być wspierana przez kod
- Model powinien odpowiadać wyłącznie za zadania, do których sprawdza się najlepiej. Pozostałe powinny pozostać po stronie kodu

Niezależnie od tego, czy będziemy budować chatboty, czy nie, interakcje z LLM warto zapisywać w bazie danych. Przykładowa struktura tabeli, powinna zawierać identyfikator wiadomości, wątku, nazwę użytkownika, asystenta bądź narzędzia, a także rolę, zapytanie oraz wygenerowaną odpowiedź. W niektórych sytuacjach może przydać się także źródło czy kontekst wykorzystany do generowania odpowiedzi. 

![Przykładowa struktura tabeli w bazie danych, umożliwiająca przechowywanie wiadomości](https://cloud.overment.com/2024-08-15/aidevs3_db-10cc2714-d.png)

O pracach z bazami danych na potrzeby aplikacji korzystających z LLM będziemy jeszcze mówić, ponieważ temat ten łączy się z silnikami wyszukiwania, w tym także z bazami wektorowymi. Całość stanowi niemal niezbędny element pracy z LLM.
## Routing i połączenie wielu modeli

Choć najlepszych LLM jest zaledwie kilka, tak praca z Generatywnym AI może wymagać skorzystania z różnych modeli, w tym także modeli po Fine-Tuningu. Powodów może być kilka: 

- **Redukcja kosztów** (mniejsze modele są zwykle tańsze)
- **Redukcja czasu reakcji** (mniejsze modele są zwykle szybsze)
- **Zachowanie prywatności** (część danych może być przetwarzana lokalnie)
- **Lepsze dopasowanie** (modele wyspecjalizowane w danym zdaniu uzyskują lepszą skuteczność)
- **Zwiększenie precyzji** (niektóre zadania mogą być realizowane przez kilka modeli, a następnie oceniane i wykorzystywane do wygenerowania ostatecznej odpowiedzi)
- **Zmniejszenie wymagań** (niektóre operacje mogą być wykonane na urządzeniu użytkownika)

Do dyspozycji mamy przynajmniej kilka platform, oferujących łatwy dostęp do wielu modeli. Są to m.in. Replicate czy OpenRouter lub usługi takie jak Amazon Bedrock czy Vertex AI. Oczywiście możemy także pracować bezpośrednio z OpenAI czy Anthropic, jednak będzie to zależało od naszych potrzeb i kwestii prawnych. Należy zwracać przede wszystkim uwagę na sposób przetwarzania danych, ceny oraz stabilność i przepustowość API. 

Wiemy już, że aktualnie każdy framework oraz narzędzia takie jak Vercel AI SDK oferują integrację z różnymi usługami i tym samym modelami. Natomiast mówimy tutaj nie tylko o samym interfejsie, lecz sposobie jego zastosowania, czego świetnym przykładem jest projekt Route LLM. Dzięki niemu możemy ocenić który z dostępnych modeli powinien obsłużyć dane zapytanie. 

Poniżej mamy wykres twórców Route LLM sugerujący, że przełączanie się pomiędzy modelami pozwoliło zredukować koszty (-85%), przy zachowaniu niemal pełnej skuteczności (95%). Należy jednak pamiętać, że obecnie koszt inferencji znacznie spada i może się okazać, że optymalizacja akurat w tym kierunku, będzie mieć mniejszy sens. 

![Route LLM pozwala na wybór modelu w zależności od bieżącego zapytania](https://cloud.overment.com/2024-08-16/aidevs3_router-1041924a-b.png)

Choć projekt Route LLM podchodzi do klasyfikacji zapytań w dość złożony sposób, opierając się o własny model, tak w naszych projektach możemy zastosować prompty klasyfikujące zapytanie lub po prostu dostosować logikę tak, aby część zadań była realizowana przez mniejsze modele.
## Strumieniowanie od praktycznej strony

Streaming pozwala na zwracanie fragmentów treści generowanej przez model, co zmniejsza czas reakcji potrzebny na wyświetlenie jej użytkownikowi. Wiemy już także, że strumieniowane są tokeny, ale w praktyce poza nimi możemy strumieniować także inny rodzaj informacji, taki jak chociażby powiadomienie o aktualnie używanym narzędziu czy etapie logiki realizowanej przez agenta. Dokładnie widzimy to w przykładzie `streaming`, który obsługuje zarówno przesyłanie kompletnej odpowiedzi, jak i pojedynczych fragmentów. 

![](https://cloud.overment.com/2024-08-18/aidevs3_streaming-40efbd4a-7.png)

Choć szybkość inferencji z czasem wzrasta i może to sugerować, że rola strumieniowania będzie spadać. Choć może się tak wydarzyć, tak z drugiej strony trzeba mieć na uwadze wzrost możliwości modeli, który przekłada się na wzrost złożoności realizowanej przez nie logiki. Pomimo tego, że pojedyncze zapytania mogą być już generowane bardzo szybko (np. z pomocą Groq), tak na odpowiedź Agenta AI nadal możemy dość długo oczekiwać. Wówczas warto na bieżąco informować użytkownika o statusie i etapie podejmowanych działań. Rola strumieniowania nadal jest także istotna w interfejsach dążących do interakcji Real-Time.

Poniższy schemat obrazuje przykładową logikę, która uwzględnia wykonanie kilku zapytań do modelu, ale dopiero ostatnia z nich jest strumieniowana do użytkownika. Nie jest to jednak jedyny przypadek, gdy przesyłanie fragmentów danych może okazać się przydatne, ponieważ może okazać się to pomocne przy przetwarzaniu danych pomiędzy usługami, np. ElevenLabs i Whisper (omówimy to szerzej w lekcji S02E02 — Obraz i wideo).

![Obsługa zarówno pełnych odpowiedzi ze strony modelu jak i strumieniowania pozwala na lepszą kontrolę logiki aplikacji](https://cloud.overment.com/2024-08-18/aidevs3_stream-f208f3b7-5.png)

Praktyczne wskazówki dotyczące strumieniowania: 

- Strumieniowanie odpowiedzi do użytkownika nie zawsze jest rekomendowane, ponieważ utrudnia to moderację / weryfikację generowanych treści
- Strumieniowanie utrudnia także korzystanie z tzw. placeholderów, które mogą być programistycznie podmieniane na alternatywne treści. Przykładowo model może odpowiedzieć "Oto treść maila:\n\n{{mail_content}}" i wówczas otrzymujemy zaledwie kilka tokenów, które są zamieniane na docelową treść.
- Podczas strumieniowania, dodatkowe informacje (np. identyfikator konwersacji czy inne metadane) zwykle przesyłane są w nagłówkach odpowiedzi (widzimy to we wcześniejszym przykładzie `streaming`) lub są zwracane jako oddzielny zestaw danych
- Strumieniowanie sprawdzi się w przypadku krótszych interakcji, na które oczekuje użytkownik. Jeśli generowanie odpowiedzi trwa dłużej, lepiej sprawdzą się zdarzenia (events)
- Strumieniowanie najlepiej jest obsługiwać przez oficjalne SDK danej usługi, ale nie zawsze będzie to możliwe ze względu na potrzebę dopasowania formatu odpowiedzi i/lub przesyłanych danych

## Strukturyzowanie odpowiedzi (JSON)

Zastosowanie LLM w kodzie niemal zawsze wymaga ustrukturyzowanej odpowiedzi, np. w formacie JSON lub YAML i mówimy tutaj o JSON Mode oraz Structured Output. Choć JSON jest znacznie bardziej popularny, to w filmie "[Let's build the GPT Tokenizer](https://youtu.be/zduSFxRajkE?t=7766)" Andrej Karpathy wyjaśnia wpływ tokenizacji na lepszą wydajność dla formatu YAML). Dodatkowo pojawiły się sygnały o tym, że strukturyzowana odpowiedź ma negatywny wpływ na jakość działania modelu, o czym możemy przeczytać w [Let Me Speak Freely?](https://arxiv.org/abs/2408.02442v1) (prawdopodobnie ma to związek z częściowym odwróceniem uwagi modelu od zadania, na rzecz strukturyzowania wypowiedzi).

Zatem w praktyce, mówimy tutaj o sytuacji, gdy:

- Instrukcja systemowa zawiera polecenie o wygenerowaniu JSON lub YAML o ustalonej strukturze na podstawie określonych danych
- LLM generuje odpowiedź, która zostaje parsowana, a poszczególne właściwości mogą zostać wykorzystane w dalszej części logiki aplikacji

![LLM mogą transformować treści do pożądanych formatów, np. JSON](https://cloud.overment.com/2024-08-19/aidevs3_json-6814ad4e-b.png)

Choć obecnie większość LLM doskonale radzi sobie z generowaniem prostych obiektów JSON, mniejsze modele Open Source nadal mają z tym problem. Przykładowo, Llama 3.1 8B zamiast wygenerować obiekt JSON na podstawie prostego polecenia, zwróciła odpowiedź informującą o niewystarczającej wiedzy. W takim przypadku aplikacja zwróci błąd, ponieważ dane nie zostały poprawnie przetworzone.

![LLM nie zawsze poprawnie generuje obiekty JSON i ma to związek z przetwarzanymi danymi, promptem oraz możliwościami samego modelu](https://cloud.overment.com/2024-08-19/aidevs3_badjson-9f2ae837-5.png)

Co więcej, nawet jeśli skupimy się na pracy wyłącznie z najlepszymi dostępnymi modelami, to nadal może zdarzyć się sytuacja w której albo **nie otrzymamy poprawnego obiektu JSON, albo jego struktura nie będzie zgodna z oczekiwaniem**. W obu przypadkach zakończy się to błędem, który może nawet nie być jawny, ponieważ LLM może zwrócić poprawną właściwość, której wartość będzie niepoprawna.

Obecnie do dyspozycji mamy dwa rozwiązania, dzięki którym LLM będzie generować obiekt JSON. Są to wspomniany JSON Mode i Structured Output. Nie są one jednak wspierane przez wszystkie platformy oraz nie gwarantują nam tego, że wygenerowany obiekt będzie poprawny. 

JSON Mode został przedstawiony przez OpenAI i jest implementowany przez kilka platform, np. Groq czy ollama. Jest to dodatkowy parametr obiektu żądania, określający format wypowiedzi.

![](https://cloud.overment.com/2024-08-19/aidevs3_format-a9a19c81-4.png)

Choć w takiej sytuacji wymuszamy zwrócenie obiektu JSON, to i tak nasz prompt musi zawierać jasną instrukcję, która poprowadzi LLM do wygenerowania poprawnej struktury. Nie jest to jednak gwarantowane i może okazać się, że w obiekcie pojawią się dodatkowe właściwości, innych będzie brakować, a inne będą mieć niepoprawne wartości. Na skuteczność generowania wpływa więc tutaj nasz prompt, przetwarzane dane, jak i sam model.

Poniżej widzimy przykład OpenAI Playground z aktywnym JSON mode, instrukcją wyraźnie mówiącą o odpowiedzi w ustalonym formacie oraz parametrem `temperature` ustawionym na `0`, ponieważ w tym przypadku nie zależy nam na "kreatywności" modelu. Przykład [można przetestować tutaj](https://platform.openai.com/playground/p/i30h7w74xwuAM1fu2zBH3DGz?model=undefined&mode=chat).

![JSON mode wymaga, promptu prowadzącego model do wygenerowania odpowiedzi w formacie JSON o ustalonej strukturze](https://cloud.overment.com/2024-08-19/aidevs3_json_mode-0bac3089-0.png)

Należy tutaj pamiętać o Prompt Injection i przykładach omawianych w S00E02 — Prompt Engineering. Choć w tym przypadku odpowiedź modelu będzie w formacie JSON, tak może wyglądać następująco:

![JSON mode sprawia, że model odpowiada w formacie JSON, lecz struktura obiektu oraz wartości właściwości zależą od promptu](https://cloud.overment.com/2024-08-19/aidevs3_json_mode_failed-60a5c3e3-4.png)

Jest to kolejny powód do tego, aby nie korzystać z LLM w przypadku oprogramowania odpowiadającego za krytyczne procesy oraz unikania zaangażowania człowieka w dodatkową weryfikację, przynajmniej do czasu pojawienia się rozwiązań tego problemu, co może wiązać się z kolejną generacją modeli językowych.

Poza JSON Mode, do dyspozycji mamy także Structured Output, który w przypadku OpenAI **zapewnia 100% pewności** (w trybie strict), że wygenerowany obiekt JSON będzie posiadał oczekiwane właściwości. Niestety nie daje nam to pewności, że uwzględnione w nim wartości będą poprawne i na to nadal wpływa sam model.

Structured Output wspierany jest przez wybrane modele OpenAI i można go aktywować zarówno w trybie normalnym, jak i `strict` poprzez dodanie parametru żądania widocznego poniżej. Jeśli chodzi o sam schemat, to mówimy tutaj o zastosowaniu [JSON Schema](https://json-schema.org/), a konkretnie jego obsługiwanego podzbioru, z którym można zapoznać się [w dokumentacji OpenAI "Supported schemas"](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas).

![Structured output wymaga parametru 'response_format' z ustawionym 'type' na 'json_schema' oraz 'json_schema' zawierającym właściwości 'strict' (true) oraz 'schema' na faktyczny schemat zgodny z obsługiwanym JSON Schema](https://cloud.overment.com/2024-08-19/aidevs3_structured-1a4721ed-7.png)

Przykładowy prompt w Playground wymaga zatem przekazania schematu w oknie dialogowym dostępnym po aktywowaniu ustawienia `Response format`. 

![Parametr response_format ustawiony na json_schema pozwala przekazać obiekt zgodny z podzbiorem opisanym w JSON Schema w celu wygenerowania pożądanej struktury](https://cloud.overment.com/2024-08-19/aidevs3_json_schema-9ccbd04f-d.png)

Co ciekawe, w przeciwieństwie do JSON Mode, w tym przypadku Structured Output pozwoliło na wygenerowanie obiektu JSON, który dokładnie zgadza się z naszymi założeniami. Choć instrukcja systemowa nie jest tutaj niezbędna, nadal odgrywa istotną rolę, ponieważ to, co "widzi" LLM, to nadal ustrukturyzowany ciąg tokenów, na podstawie których generowana jest odpowiedź.

![Przykład działania Structured Output](https://cloud.overment.com/2024-08-19/aidevs3_schema-38a842f9-a.png)

W poniższym przykładzie widzimy Structured Output pobierający datę wspomnianą przez użytkownika w formacie YYYY-MM-DD. Mimo że użytkownik wspomniał jedynie słowo "jutro / tomorrow", model poprawnie określił docelową datę, korzystając z kontekstu dostępnego w instrukcji systemowej.

![Structured Output bierze pod uwagę nie tylko informacje dostępne w JSON Schema, ale także w instrukcji systemowej](https://cloud.overment.com/2024-08-19/aidevs3_system-665877ea-b.png)

Obiekt zawierający odpowiedź asystenta, może zawierać dodatkowy parametr `refusal`, który zostanie ustawiony na `true` w przypadku braku możliwości wygenerowania oczekiwanego formatu JSON. Dzięki niemu możemy obsłużyć takie sytuacje w logice aplikacji. 

![W przypadku braku możliwości wygenerowania obiektu JSON, LLM może odmówić wypowiedzi, informując o tym w parametrze 'refusal'](https://cloud.overment.com/2024-08-19/aidevs3_refusal-2393d47e-3.png)

Podsumowując, Structured Output:

- Strukturyzowane odpowiedzi są wspierane przede wszystkim przez OpenAI, natomiast pozostałe platformy i narzędzia stopniowo również je wprowadzają
- OpenAI wspiera tylko część opcji dostępnych w JSON Schema i dodatkowo zawiera [ograniczenia opisane w dokumentacji OpenAI](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas) związane ze strukturą obiektu czy niektórymi ustawieniami
- Nazwy (`name`) i opisy (`description`) właściwości opisanych w JSON Schema powinny być napisane z myślą o LLM, który na ich podstawie będzie określać ich wartości
- Structured Output w trybie `strict` będzie miało zwiększony czas reakcji dla danego schematu, ale tylko za pierwszym razem. Jest to uzasadnione koniecznością przetworzenia struktury przez serwery OpenAI
- W sytuacjach gdy struktura obiektów JSON nie jest nam dokładnie znana lub musi być bardziej elastyczna niż pozwala na to Structured Output, lepszą opcją będzie skorzystanie z JSON Mode
- Odpowiedzi w formacie JSON są **niezbędnym elementem integrowania LLM z logiką aplikacji**.
## Formatowanie odpowiedzi po stronie front-endu

LLM może generować odpowiedzi ustrukturyzowane nie tylko w formacie JSON czy YAML, ale także Markdown. To umożliwia łatwą konwersję do HTML i poprawne wyświetlenie po stronie interfejsu użytkownika. Samo konwertowanie tych formatów nie jest czymś nowym, jednak w połączeniu ze strumieniowaniem, interfejsem czatu oraz możliwościami modelu, temat ten się komplikuje. Mowa tutaj konkretnie o między innymi:

- Tempo strumieniowania odpowiedzi sięgające nawet kilkuset tokenów na sekundę oraz uruchamianie funkcji formatujących tekst przy każdym tokenie negatywnie wpływa na responsywność interfejsu. Dlatego należy dbać o to, aby przetwarzać możliwie małe zestawy danych (np. ostatnią wiadomość na czacie lub jej wyodrębniony fragment)
- Model może generować zaawansowane formaty treści, np. kod źródłowy, tabele czy składnię LaTeX, które wymagają dodatkowego formatowania po stronie front-endu
- W przypadku składni Markdown konwertowanie na HTML może być realizowane przez bibliotekę [Marked](https://marked.js.org/), [remark](https://github.com/remarkjs/remark), [commonmark](https://commonmark.org/) lub [markdown-it](https://markdown-it.github.io/)
- W przypadku kodu źródłowego przydatna jest biblioteka [highlight.js](https://highlightjs.org/) 
- LLM może generować także diagramy dzięki narzędziom takim jak [Mermaid](https://mermaid.js.org/) czy [Markmap](https://markmap.js.org/)
- LLM może generować także dynamiczne komponenty dzięki Vercel AI SDK oraz dostępnej w nim opcji `Generative UI`
- Strumieniowanie wypowiedzi modelu utrudnia pracę z tagami takimi jak markdown block, czyli \`\`\`. Problem polega na tym, że tag ten w trakcie strumieniowania musi być zamknięty, lecz kolejne tokeny muszą być zapisane wewnątrz niego
- Strumieniowanie treści może trwać nawet kilkadziesiąt sekund, a jej aktualizowanie po stronie front-endu może powodować ponowne renderowanie komponentu, co uniemożliwia jej zaznaczenie czy skopiowanie
- Strumieniowanie odpowiedzi powinno być możliwe do przerwania z poziomu interfejsu użytkownika, co powinno zatrzymać żądanie po stronie serwera. Jest to możliwe dzięki [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController) 

![Odpowiedzi generowane przez LLM mogą być zwracane w formacie markdown a ten zamieniany na HTML](https://cloud.overment.com/aidevs_streaming-1724091505.gif)

Choć implementacja renderowania treści generowanych przez LLM będzie się różnić w zależności od aplikacji, zastosowania i narzędzi, to ogólny schemat prezentuje się następująco: 

![Strumieniowanie tokenów z serwera do interfejsu użytkownika wymaga parsowania formatu markdown do HTML](https://cloud.overment.com/2024-08-19/aidevs3_content-7ca61548-a.png)

A więc kontakt z LLM **zawsze odbywa się po stronie serwera**, ponieważ wymagane jest uwierzytelnienie z pomocą klucza API, który **nie może być przechowywany po stronie klienta**. Następnie po stronie aplikacji front-endowej treść generowana przez model musi być przechowywana zarówno w swojej oryginalnej formie (plain text / markdown), oraz sparsowanej (HTML & CSS).
## Dobre praktyki bezpieczeństwa

Niedeterministyczna natura LLM, wczesny etap rozwoju modeli oraz fakt, że pracując z nimi mamy do czynienia z naturalnym językiem, który jest mniej precyzyjny niż języki programowania, musimy pamiętać o istotnych kwestiach bezpieczeństwa.

- **Prywatność danych:** Pracując z komercyjnymi modelami, musimy dbać o prywatność danych, w czym mogą pomóc plany Enterprise oraz usługi oferujące dostęp do modeli w chmurze (np. Amazon Bedrock czy Vertex AI)
- **Prompt Injection** Wszystkie informacje trafiające do promptu powinny być uznawane za publicznie dostępne
- **Limity kluczy API:** Klucze API wykorzystywane w aplikacji powinny mieć ustawiony twardy limit budżetu na wypadek uruchomienia zapytań do modelu w pętli
- **Limity API** Produkcyjna aplikacja powinna narzucać limity zapytań dla użytkownika oraz chronić API przed atakami DDOS
- **Weryfikacja:** Działanie modelu i generowane treści powinny być nadzorowane i weryfikowane przez człowieka
- **Ograniczenia**: LLM połączony z logiką aplikacji powinien mieć narzucone programistyczne ograniczenia, uniemożliwiające usuwanie i nadpisywanie informacji
- **Kopie zapasowe**: W przypadku konieczności nadania uprawnień związanych z modyfikacją danych (np. integracja z systemem CRM), konieczne jest prowadzenie pełnej historii zmian na potrzeby ewentualnego przywrócenia danych
- **Moderacja danych wejściowych**: W przypadku pracy z OpenAI wszystkie treści przekazywane przez użytkownika powinny być filtrowane przez Moderation API
- **Moderacja danych wyjściowych**: Moderacji mogą podlegać zarówno treści trafiające do LLM, jak i przez niego zwracane (w tej sytuacji rezygnujemy ze strumieniowania)

Inaczej mówiąc: LLM powinny mieć ograniczone uprawnienia dostępu do danych, akcji oraz być chronione przed nieautoryzowanym dostępem bądź niewłaściwym zastosowaniem ze strony użytkownika.

## Podsumowanie

Integracja LLM z aplikacją otwiera scenariusze, które trudno osiągnąć pisząc kod samodzielnie. Jednak oddawanie im zbyt dużej odpowiedzialności jest nie tylko mało efektywne i kosztowne, ale może generować problemy czy nawet narażać na utratę danych. Z tego powodu powinniśmy rozwijać umiejętność oceny tego, do jakiego stopnia chcemy wykorzystywać modele w naszej aplikacji lub czy w ogóle chcemy to robić. 

Z tej lekcji kluczową umiejętnością jest zdolność do wieloetapowej interakcji z LLM poprzez SDK lub bezpośrednie zapytania do API, a także generowanie ustrukturyzowanych odpowiedzi z pomocą JSON Mode i/lub Structured Output z uwzględnieniem możliwości korzystania z kilku modeli.