![](https://cloud.overment.com/0103-1730871460.jpeg)

While generative AI models produce remarkable results, we lack clarity on whether they exhibit true reasoning or simply reproduce patterns from their training data. [Ilya Sutskever](https://x.com/ilyasut) and [Geoffrey Hinton](https://x.com/geoffreyhinton) suggest that "effective prediction of the next token requires actual understanding of the content." Conversely, [Yan LeCun](https://twitter.com/ylecun) argues that these models exhibit only primitive reasoning mechanisms, far removed from human brain processes. Additionally, some voices on the internet claim that large language models are a scam, hindering technological development.

Below, we have a "puzzle" that only seemingly resembles the [popular conundrum](https://en.wikipedia.org/wiki/River_crossing_puzzle) of crossing the river. Normally, its difficulty depends on the limitations we must adhere to. However, in our case, there is no mention of any of them. Despite this, the model is unable to notice this and suggests a complicated list of steps.

Such behavior suggests that we are not dealing with reasoning here but following the patterns of training data. We see the same in another example, where the model "claims" to be able to use the [MD5 algorithm](https://en.wikipedia.org/wiki/MD5) and actually successfully encodes the word "Hello." However, if we slightly change it to "H3llo," the generated result is incorrect.

Problems like these apply not only to large language models but to generative AI models in general. Below we have an image of an "empty room with no elephant" generated by Dall-E 3.

Again, we see behavior here suggesting that we are not dealing with intelligence here. So how can we trust models if we can't rely on them in such simple situations?

From a practical point of view, it is difficult not to see both the broad possibilities and glaring flaws. Moreover, we're not just talking about the skills of the models themselves but also the limitations associated with their technology or infrastructure. To emphasize what I am talking about now, this is the status of Anthropic services over the past 90 days. One can only add that 99.43% service availability is a very optimistic value.

Those trying to defend generative AI claim it is "just the beginning" and that models are still evolving, and all problems will be solved over time. It's hard to say at the moment if this will actually happen, but the overall progress is well represented by the development of Midjourney, which can also be juxtaposed with a model like [Flux](https://www.3daistudio.com/blog/FLUX-Image-Generator-for-3D-Models) or [Recraft v3](https://replicate.com/recraft-ai/recraft-v3).

Even though we don’t know what the next versions of models will bring us and how tools will develop, examples of products like Cursor, [Replit Agent](https://docs.replit.com/replitai/agent), or Perplexity suggest that even if generative artificial intelligence were to completely stop evolving today, we still have the foundations to build value-generating solutions.

We can, therefore, choose to adopt a strategy of working practically with models and personally experiencing the possibilities associated with them. We can already successfully look at them as tools that are only useful in selected scenarios. Hence instead of trying to apply them everywhere possible, it is wiser to use them only where they can actually be helpful.

And that's what we're going to talk about today.

## Basic Model Limitations

The Transformer model, which forms the basis of large language models, was originally created for **translating content between languages**, which makes it particularly good at **transforming existing content**. Its key element is the attention mechanism (wonderfully explained in the video https://www.youtube.com/watch?v=eMlx5fFNoYc), thanks to which the model maintains focus on essential pieces, retaining context and existing connections.

Familiarizing oneself with even the general mechanisms of the Transformer model allows us to understand that we are dealing with a mechanism mimicking the human brain, not a human brain itself. This means that it will be better than a human in some tasks and worse in others.

In recent years, companies developing large language models have refrained from publishing details about their architecture. However, with quite high probability, we can suspect that models like GPT-4 use the "[Mixture of Experts](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/)" concept. It assumes that a language model consists of networks specialized in specific tasks, and one of them (the router) is responsible for selecting the ones that best fit the given task.

Probably (I have no proof of this) because of this, the model's effectiveness drops for certain tasks when we require responses in a specific format, e.g., JSON Mode, as described in [Let me speak Freely?](https://arxiv.org/abs/2408.02442v1). It can be suspected that in such a situation, the model's attention focuses more on the method of recording than on the actual task. This may lead to the conclusion that it is justified to break down tasks into smaller stages. Besides, it also helps in maintaining focus on the main instructions.

And in all of this, we must also remember that large language models learn about the world based on data (initially only text, now also through other formats). Therefore, it is more challenging for them to understand what we, as humans, learn through experience. The situation is not helped by the fact that instead of words, they use tokens, which also contribute to many problems.

The above issues related to the architecture of models directly relate to their capabilities and limitations. It is good to keep them in mind when designing applications and prompts, which I try to demonstrate in each of the examples presented.

To summarize this section:

- It's easier to **transform existing content** than to generate new.
- It's easier to **verify content** than to transform it.
- Models learn about the world through content, not through all senses.
- Mechanisms of maintaining attention, context recognition, and associations are impressive, yet they have their shortcomings.
- "Predicting the next token" requires some degree of content understanding, which requires generalization/compression of information, which in turn relates to the ability to notice patterns.

## Costs, Rate Limit, and Debugging

The limitations of model capabilities are not the only challenge. Since using language models is chargeable and billed based on the processed tokens, it is necessary to control expenses, especially within a team. Until recently, setting limits was not possible at the provider level but involved building one's own solution to restrict API key access. Now almost all services offer some level of cost control from the panel (this also applies to Anthropic and OpenAI), either considering the project itself or individual users.

Setting limits for the user is critical as a simple mistake in the code can lead to exhausting the available budget already at the development stage, not to mention production.

Depending on the situation, a good solution is to use cheaper versions of models where possible, whose prices are already very low (e.g., [Gemini Flash](https://github.com/google-gemini/gemini-api-quickstart)).

Once the application is made available to users, besides costs, the "rate limit" becomes a problem, i.e., restrictions on the number of API requests, which usually encompass:

- the number of tokens per minute/day
- the number of requests per minute/day

Limits vary depending on the model, provider, and account level (so-called tier), which increases over time. When faster access to higher limits is essential, it is possible to contact customer service for individual consideration and/or use services like Amazon Bedrock or Azure OpenAI Service. Anyway, spending limits and API access must be taken seriously because once an application is released to production, it is challenging to address this issue in a short time.

Apart from rigid limits, when working with language models, we are also interested in actual token usage and the cost estimation of the application's operation. Currently, the best solution for this purpose is to use monitoring tools, such as the already mentioned LangFuse, [LangSmith](https://smith.langchain.com/), [Portkey](https://portkey.ai/), [Parea](https://www.parea.ai/), or others (I personally use LangFuse). Thanks to them, we have insight into both general token processing statistics and tokens required to perform individual queries.

Connecting to LangFuse is possible either directly via API or SDK (we must handle transmitting a complete set of monitoring information ourselves) or through available integrations, which relieve us of some duties. Although in the [`langfuse`](https://github.com/i-am-alice/3rd-devs/tree/main/langfuse) example we use the JavaScript interface, the essential thing to understand is only the concept, which involves creating several methods with which we will send events. I discuss the details of the LangFuse platform in the video below.

[Watch the video](https://player.vimeo.com/video/1008437926?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479)

Also, to summarize, linking the application with Langfuse involves sending data for each query, considering interactions with the model and actions related to model operation. In other words — we send everything that will help us understand the model's behavior. Particularly useful is the ability to debug prompts, which I also mentioned in lesson S00E04 — Programming.

## Token Window Limits

Although we have already covered monitoring the number of processed tokens and related costs, token calculation itself is a process that still interests us. Specifically, having this information in the application code, which is essential both due to the Context Window limit discussed in lesson S00E02 — Prompt Engineering and when working with documents whose size we will want to know.

From previous editions of AI_devs and onboarding materials, the Tiktokenizer tool should already be known to you. Now, we must use it in the application code to correctly count tokens.

For OpenAI, we have a new library available directly from Microsoft, called [Tokenizer](https://github.com/Microsoft/Tokenizer). In its source code, we find a list of encoders used by different models, which help us find out the number of tokens for specific data.

Besides the encoder, we also need to consider special tokens responsible for structuring the content passed to the model. Specifically, the tokens visible below, including the < | im_start | > tags and keywords such as `system` or `user`.
In the example [`tiktokenizer`](https://github.com/i-am-alice/3rd-devs/tree/main/tiktokenizer) in the file `OpenAIService.ts`, there is logic responsible for counting tokens for the provided list of messages and specific model. This includes the mentioned special tokens, which may also vary depending on the model and its encoder.

Counting tokens also applies to images, but in this case, we rely not on a tokenizer but on rules described on the [OpenAI](https://platform.openai.com/docs/guides/vision/calculating-costs) page or other provider instructions.

We will come back to this topic several times during AI_Devs 3. In the meantime, run the example [`tiktokenizer`](https://github.com/i-am-alice/3rd-devs/tree/main/tiktokenizer) and send a sample query to it, then compare the result with the Tiktokenizer tool available online.
## Generating Content Limits

In the lesson S00E02 — Prompt Engineering, we discussed the token limit in model output, which for the GPT-4o model amounts to only 4096 tokens. For Claude models, we can already achieve up to 16,000 tokens, but from a practical standpoint, this may still not be sufficient. It is enough to want to introduce corrections, translations, or any other transformations on text that exceeds the model's limit in terms of volume.

In such a situation, we have several options:

- split a long text into smaller segments, each shorter than the permissible limit for the model's expression
- programmatically detect the reason for the model's expression ending and ask for continuation

We will deal with the first solution later in the course, while the second can already be seen in the example [`max_tokens`](https://github.com/i-am-alice/3rd-devs/tree/main/tiktokenizer). There, I deliberately limited the length of the model's expression to only `50 tokens`. If we send a query with the message "**Write ten sentences about apples and put them in order**", the task will not be performed correctly by default and will end with information about exceeding the value of [`max_tokens`](https://github.com/i-am-alice/3rd-devs/tree/main/tiktokenizer), as seen below.

We can therefore programmatically detect this reason and automatically continue the conversation, appending a request for further expression, beginning from the last character of the previous message.

Although the first strategy related to breaking content into smaller segments is more effective, the above scenario can also be considered. However, it will only work in cases of models where we can count on precise instruction-following (e.g., GPT-4o).

In the example [`max_tokens`](https://github.com/i-am-alice/3rd-devs/tree/main/tiktokenizer), it is also worth noting the file `app.ts`, where there is logic checking whether the sum of tokens in the prompt and the model's output exceeds the context window limit. Such a query would still result in an error from the API side, but it's worth remembering to count the prompts precisely and take token limits into account for the model we are working with.

## Imposing Own Constraints

Not all constraints will result from the model's or tools' limitations but from our own needs. After all, we may want the model to refuse to perform selected tasks or strictly adhere to guidelines described in the prompt.

Here, however, it's essential to remember that in the case of LLMs, we are only talking about the possibility of controlling the model's behavior, not full control. Moreover, processing natural language means that it's relatively easy to find workarounds for certain rules, as discussed in lesson S00E02 — Prompt Engineering and the techniques of Prompt Injection or Jailbreaking.

The first example of constraints we may want to use is the Moderation API available in OpenAI. Although it verifies content for compliance with the company's policy, it allows filtering various categories of undesirable content, such as violence.

The application of the Moderation API involves merely sending one query [described in the documentation](https://platform.openai.com/docs/guides/moderation/quickstart), so we will not dwell on it. Nevertheless, it's worth knowing that moderation is handled by the model `omni-moderation-latest`, which has a limited number of tokens and is susceptible to jailbreaking. This suggests that at some point, we might be interested in building our own model that will evaluate queries according to our regulations and rules.

A more flexible strategy for imposing our own constraints is to introduce additional **assessing** and/or **verifying** prompts, whose task will focus solely on assessing the user's query and/or the model's expression, according to our own rules. Interestingly, by introducing our own rating scale, we can programmatically block queries trying to overwrite the logic of our prompt.

Specifically, in the example [`constitution`](https://github.com/i-am-alice/3rd-devs/tree/main/constitution), there is an example of user query verification. It is checked for whether the message was written in Polish. The model's task is to return the word `block` or `pass`, which is then programmatically verified using the conditional `if` instruction.

This means that if anything disrupts the operation of our prompt and the returned value is not exactly `pass`, the query will be rejected.

We could also launch a similar prompt on the response returned by the model to add another moderation layer. Furthermore, we talk here about **completely separate prompts** that are executed in the background, so the user does not physically have access to them.

Unfortunately, this is not a perfect safeguard against prompt injection because the content sent to the model can be fabricated to bypass it. Besides, it may turn out we accidentally block queries that in no way break our rules but are incorrectly assessed by the model.

However, it does not change the fact that evaluating generated content is a good way to increase the stability of application operation. It can be used not only in a security context but also in evaluating the results returned by the model itself. As I said — **it is easier to assess content than to generate it.** As a result, we can support the model's reasoning in this way.

Before we move further, I'll add that in the assessing prompt, it is very advisable to add space for "consideration." We can do this either by expecting a JSON format or by using the format below. It involves using the `<thinking>` and `<result>` tags, where the model can input the expected content, and then using a regular expression, we can obtain the result.

In the `<thinking>` block, the model generates justification, gradually **increasing the likelihood** that the next tokens will be generated following our rules. This is one of the best techniques for strengthening the model's reasoning, especially when combined with result evaluation. You just have to ensure that "show the model how to think," which means providing several examples of "thinking" block content. Otherwise, it usually generates low-value content there.
## Model Performance

Until recently, LLMs were the slowest element of an application, which was a significant limitation. However, this is beginning to change for at least two reasons — the development of small models capable of operating on mobile devices and the development of inference equipment provided by platforms like Groq or [Cerebras](https://cloud.cerebras.ai/). Unfortunately, none of them yet has a purchasable plan on the site.

In the [LLMPerf Leaderboard](https://github.com/ray-project/llmperf-leaderboard?tab=readme-ov-file) repository, statistics are led about popular platforms characterized by inference speed. However, an equally important indicator is `time to first token`, that is, reaction time. Besides, some of these services also impose quite aggressive limits, which negatively affects task execution time.

In one of the early examples of the "completion" onboarding material, we analyzed several tasks for labeling. To optimize realization time, all classifications were launched in parallel. This approach indeed increases application performance but makes us vulnerable to exceeding query limits and the limit of processed tokens over time.

Limits are usually significantly larger for smaller models, which also operate much faster than more powerful versions. Therefore, in the context of optimizing application performance, it's worth asking questions such as:

- How can we design logic to perform as many queries as possible in parallel?
- Can we use a smaller, faster model, even at the cost of more extensive prompts?
- Can we use prompt caching to decrease response time (e.g., in the case of Anthropic models)?
- Can we make use of platforms offering fast inference?
- Will performance even matter to us, as, for instance, some tasks can be performed in the background?
- Does every task need to be executed by the model, and can we at least move some logic to the code (e.g., through regular expressions)?

In the end, it's worth adding that inference speed increases over time. However, it's possible we will observe cycles where new models will be slower, and sometimes they will be optimized, and their speed will increase. Although the below image may not fully reflect the actual state, it certainly visualizes the recent years of large language model development.

![](https://cloud.overment.com/2024-09-12/aidevs3_cycles-8eebe210-4.png)

## Uncensored Models

Large language models inherently have several limitations and constraints, not due to their nature, but the actions taken by their creators for safety. Such restrictions are also present in Open Source models, but among them, a group of `uncensored` models is emerging that does not have these constraints or can easily be bypassed. Examples of such models may be Dolphin, created based on other models (e.g., Llama or Mistral) by the company [Cognitive Computations](https://x.com/cognitivecompai) or Grok created by [x.ai](https://x.ai/).

Uncensored models may associate with the ability to generate content generally considered inappropriate or answering questions that may constitute threats from a safety perspective. This is true, but also censorship can delve into areas we want to address without bad intentions. It is easy to imagine that the topic of uncensored models in itself is controversial, and [it's worthwhile to learn the perspective of the person behind the Dolphin model series](https://erichartford.com/uncensored-models).

Below we have the difference in the behavior of the `Claude 3.5 Sonnet` model and the `dolphin-llama3:70b` model. In the case of the former, generating a conversation between two politicians ended with a refusal, while the latter had no problem with it.
![Image 0](https://cloud.overment.com/2024-09-12/aidevs3_censored-fd00752d-b.png)

![Image 1](https://cloud.overment.com/2024-09-12/aidevs3_uncensored-3ca3b75d-d.png)

At this moment, one might think of the problem of bots spreading misinformation on the Internet or deceiving people in private messages. These are real threats that everyone experiences to some extent.

On the other hand, there are business situations that cannot by default be addressed by censored models. An example might be the editing of books in the crime or thriller categories, where one might encounter phrases and scene descriptions blocked by, for instance, a Moderation API.

Of course, it's worth maintaining prudence and working responsibly with uncensored models. In the case of commercial models, most unwanted content is blocked, but here we must take care of it ourselves.

As for running models like Dolphin, they are currently available via Ollama and Hugging Face, and working with them is no different from other models. The exception is the system prompt, which should emphasize allowing certain behaviors, including the style of expression.

## Summary

Over the past several months, the capabilities of large language models have continually increased, either due to the release of new versions or due to new design techniques for prompts. Similarly, various parameters, such as context window limits or the number of generated tokens, have increased.

It is indeed difficult to compare the comfort of working with models, between what we have now and what we experienced a year or two ago. Despite this, today's lesson has shown us that limitations still exist, and some of them we will even want to maintain. So, summarizing the topic of limitations:

- Monitoring applications, processed tokens, and costs is **critical** from both a technological and a business perspective
- Controlling the number of tokens for the processed content, as well as the request limit, will also help us avoid unnecessary costs. Here we’re talking about using `tokenizer` settings for the current model
- Platform limits (speed, rate limit, response time, stability) are a major problem in production. And while the situation is improving month by month, it should be taken into account at the early planning stage
- Moderating content entering the model and content generated by the model is a process that does not provide 100% security and predictability, but significantly enhances the quality of application performance
- Optimizing application performance involves design changes, allowing model queries to be executed concurrently or in the background.
- Not all tasks need to be accomplished with the help of the best model
- Not all tasks require engagement from **any model**

If you do just **one thing** from this lesson, make sure to watch the video on LangFuse and run the example with the same name (`langfuse`) on your computer.