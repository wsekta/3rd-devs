![](https://cloud.overment.com/S01E02-1725905893.png)

Throughout our experience, we've often created prompts that [guide the "dream" of the model](https://twitter.com/karpathy/status/1733299213503787018?lang=en) through data provided to the system instruction. This allows for extending the model's knowledge and even teaching it new skills, such as classification.

External data can be manually entered into the system prompt, but they can also appear automatically, as exemplified by the commonly known RAG, or Retrieval-Augmented Generation.

In generative applications, the context typically comes from a database, an external API, file contents, or from all these sources simultaneously. Anyone who has developed their first applications connecting the model with external data sources knows that it offers intriguing possibilities but also poses a number of challenges, which we will partially address in this lesson.

In lesson S00E04, we discussed a `completion` example aimed at classifying tasks into one of three categories - **work / home / other**. It was not very practical since tasks are usually assigned to one of our projects or predefined labels. I personally manage tasks in [Linear](https://linear.app/), and the actual list of categories for me includes:

In most cases, new tasks are automatically added based on messages I send to my AI Agent. Nevertheless, sometimes I manually write them in Linear. At that point, a webhook is triggered, where the code similar to example [`linear`](https://github.com/i-am-alice/3rd-devs/tree/main/linear) automatically assigns the project.

The `assignProjectToIssue` function is particularly interesting, as it not only selects the project but also **programmatically verifies the correctness of the identifier** and sets its value to default if necessary.

Example [`linear`](https://github.com/i-am-alice/3rd-devs/tree/main/linear) shows us, therefore, that programmatic use of large language models extends beyond chat interfaces. In this case, the classification process took place only **in response to the user's action** and not their direct message. Moreover, classification occurs only if the user hasn't set the project themselves (and this is also programmatically checked).

So even if the problem of the non-deterministic nature of models is hard to solve, we can still take advantage of their capabilities to **support existing activities** or **partially autonomously** carry out a process.

In the above automation, information about available projects and rules for selecting them is **manually placed in the prompt** in the `context` section.

It won't always be like that. Instead, the context will be loaded **dynamically** from multiple sources, often requiring additional content transformation. But let's start from the beginning.

## Recommended Data Exchange Formats

Large Language Models cannot read binary files such as PDF or DOCX, so we need to convert them to a comprehensible form. Depending on the document, this may mean losing formatting, which translates into the risk of incorrect content interpretation. Even the presence of an image or an external link complicates the understanding of the file's contents. Any associated ambiguity directly increases the risk of hallucination and reduces the effectiveness of our system.

If you have experience processing PDF documents, you know it's difficult to talk about a universal parser. But building a tool to recognize specific templates or find specific information is usually possible. The presence of large language models pushes the boundary of what's been considered possible, but it doesn't solve all problems.

An example is the screen capture below from the AI_devs 3 lesson, saved in [notion](https://www.notion.so/). Thanks to [notion-to-md](https://www.npmjs.com/package/notion-to-md), we can download it in the open markdown format. This suggests, therefore, that its entire content, including images, will be accessible to LLM.

It turns out, however, that it isn't because loading attachments requires logging in, which a language model is not inherently capable of doing. The image below is just one example that **we always want to precisely verify whether we have unrestricted access to the content.** In the case of Notion, this is possible by generating a temporary link, but it won't always be like that.

Reaching the content is not the only problem we face. We'll also use LLM for **transforming existing documents**, which is again difficult with binary formats (like PDF).

So, from the initial stages, we must answer questions:

- **Source:** Where do the data come from and how often do they change? Will they be read directly from the source, or do we need to store their versions on the application side and regularly update them? Or maybe the data will be created, and we have significant control over their structure?
- **Organization:** What does the data structure look like, including relationships, e.g., with the source, users, and other data? What beyond the main content should we know about it, and what should the model know?
- **Access:** For whom will the data be available, and how will we search and filter it? What tools will be involved in this process?
- **Delivery:** How will the data be presented to the model, and will this process be divided into several stages (e.g., to summarize a large document)? Will the data be processed individually or in conjunction with other information?
- **Presentation / Record:** What happens to the result returned by the model? In what format will they be recorded and/or presented to the user? Besides the model's response, do we need to save anything else?
- **Modification**: Should the original data be overwritten? How will we be able to reverse the model's operation? What constraints does the data format impose on us? How can we mitigate the risk of errors (e.g., through human supervision)?

The above questions lead us to conclude that it's worth striving to work with open formats such as **markdown, txt, json, or yaml**, as well as directly with databases. Projects where binary formats come into play or access to information is hindered for other reasons will need to specialize in specific tasks. Sometimes it may turn out that after initial verification, a given project is unprofitable for further implementation.

## Transformation and Compression of Content

Example `websearch`, which we discussed in lesson S01E01 â€” Interaction allowed connecting a large language model with search results on the Internet and selected websites.

Originally, the content of a web page is saved in HTML format and contains a number of unnecessary (from the model's perspective) tags. Thanks to FireCrawl, we immediately obtained a cleaned Markdown structure, but in practice, we will often carry out similar transformations ourselves.

For example, a PDF document (with a simple structure) can be converted to HTML, and HTML to markdown. In such a form, the content can be modified by LLM, which can format statements using this syntax. We then reverse the process to obtain the original PDF format.

This mechanism won't work for complex PDF structures, but the very concept of format conversion may prove useful in other situations. One of them could be generating a YAML format instead of JSON, as mentioned by Andrej Karpathy in the [Let's build tokenizer together](https://www.youtube.com/watch?v=zduSFxRajkE) video, indicating that YAML syntax might be considerably more friendly to the model due to the tokenization process.

For a simple JSON object, we're talking about a 30% difference in tokens (according to Tiktokenizer and the GPT-4o model), which the model doesn't have to generate if we record the data in YAML format. This also translates into lower costs and shorter inference time.

Therefore, when working with various data formats, it's always worth asking ourselves first whether we can transform to a form that's more model-friendly. This applies equally to images, audio files, and video, which we will discuss further.

Transforming and cleaning data is not just a matter of token economy but also managing the model's attention, which need not be dispersed over unnecessary information. Although the ability to manage attention is constantly being optimized, it's important to keep this in mind, especially for Open Source models.

We also have different types of compression at our disposal, which can be performed programmatically or with the help of a language model. The most classical example one can find on the Internet is **dividing a document into smaller fragments (known as chunking)**. The problem is that splitting a file can cause us to lose the essential context, leading to hallucinations or generating correct but incomplete answers.

Instead, we can conduct more advanced processing of the file's content, analyzing it entirely, often multiple times, to generate new data. This pertains to creating notes on discussed concepts, definitions, or issues. As a result, instead of matching the user's query with the original file's content, the role of context is taken by the generated notes. An example of this approach can be seen below, where an overall summary and a list of concepts are generated from the uploaded file, which form the query context.

If we don't make significant errors in the file processing phase, the risk of omitting essential data is less than in the case of slicing the document into fragments.

In summary of content transformation:

- It's worth striving to work with open formats, whenever possible. The markdown syntax and JSON format (or YAML) are the most flexible, and their popularity allows us to use them in combination with external systems (e.g., CMS or email clients).
- When working with external data, we don't need to assume in advance that they will reach the model in their original form. The fact that we have access to a large language model allows us to preprocess them beforehand.
- Despite the great flexibility large language models offer, it's still worth thinking about possibly specializing solutions and systematizing data sources. Though it may change in the future, it's hard to load an entire knowledge base into a system prompt and expect quality answers. Instead, we'll be inclined to work only with data essential for generating a response at any given moment.

## Dedicated Knowledge Sources for LLM

Fitting applications to external data sources is not the only strategy we can consider. Sometimes it will be more justified to build a knowledge base from scratch with LLM in mind. However, this doesn't mean that everything has to be manually written by a human, as they can only verify content generated by the model.

This refers to situations where effective transfer of document content to the model won't be possible. In this case, we go through a partially automated content processing process once. When the data we work with doesn't change too often, such an approach may be justified.

Alternatively, the knowledge base can be generated during interaction with the application. One implementation of this approach is the [mem0](https://github.com/mem0ai/mem0) project. Its premise is to dynamically remember information for ongoing interaction or tasks carried out in the future. This is particularly valuable for AI agents, albeit currently difficult to talk about their fully autonomous operation.

In example [`files`](https://github.com/i-am-alice/3rd-devs/tree/main/files), there's assistant logic capable of remembering conversation history and creating its own memories. This mechanism is quite simple and, for convenience, does not use a database but instead saves knowledge in markdown files and uses what's called a 'vector store' named [faiss](https://github.com/facebookresearch/faiss) for searching, which we will replace with a vector database, e.g., Qdrant, in the future. **NOTE:** If you know nothing about vector databases, for the purpose of this example, just think of them as a search engine.
The example extends interaction with the model by **searching available knowledge** and adding it to the context of the main system prompt. Additionally, it includes an extra step that allows for saving the current conversation and dedicated memories. Therefore, we are not talking about connecting to an already existing data source, but building it from scratch with the assistant in mind.

![](https://cloud.overment.com/2024-09-09/aidevs3_files-07f43abf-b.png)

The operation of the example [`files`](https://github.com/i-am-alice/3rd-devs/tree/main/files) is visible in the video below, but I encourage you to run it yourself and talk to the assistant. It will quickly become apparent that the saved memories are duplicated, and once more information is memorized, it will not be possible to retrieve them all.

<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1007617435?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="01_02_learn"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

The reason for problems with saving and retrieving information lies in the implementation itself, which currently does not account for **entry updates** or an advanced memory "recall" logic.

The assistant's memories are stored in the `context/memories` directory, which can be opened using the [Obsidian](https://obsidian.md) application, where the visualization with an interactive graph is available. After the first few messages are exchanged, one can form an initial intuition about potential solutions and strategies for organizing information. This is valuable because, in a similar way, we will build a system of skills and long-term memory for AI agents.

![](https://cloud.overment.com/2024-09-09/aidevs3_map-08355b71-e.png)

The graph visualization is not accidental here, as large language models can navigate it, gathering information necessary for conversation or the current task. Unfortunately (at least for now), the challenge remains the risk of duplicating entries and generating a dynamic graph structure, which can be read about in the blog post "[Constructing Knowledge Graphs From Unstructured Text Using LLMs](https://neo4j.com/developer-blog/construct-knowledge-graphs-unstructured-text/)" on Neo4J's blog. For this reason, we will usually aim to navigate within a predetermined schema.
## External Sources of Knowledge

In the example [`websearch`](https://github.com/i-am-alice/3rd-devs/tree/main/websearch), we read data from the Internet. We did this each time, so the data was always up-to-date. However, when datasets in the form of documents, product databases, or catalogs come into play, we must ensure their synchronization. Therefore, we must always save the **original identifier or a link to the source**. Alternatively, we can generate our own identifiers as long as the content remains correctly linked.

Below, we have an example of **connecting to blog-published articles**, whose content should reach the model. Each of the entries is quite extensive and cannot be entirely included in the context of the system prompt. Moreover, we must also have the ability to search these contents. Therefore, it is necessary to process the entries and save them in a local database and/or add them to a search engine index.

![](https://cloud.overment.com/2024-09-09/aidevs3_index-fcbf4cae-8.png)

Such a system requires setting a schedule according to which new entries will be retrieved, or webhooks that will notify our application of changes on the blog.

To avoid misunderstanding, let me explain:

- On the blog, we have a **complete article** in a human-friendly format.
- For the LLM, we must adapt this format, which will differ depending on the intended purpose. For example, if we are building a tool that translates the article's content, we need to split it into smaller fragments and process them individually. The reason is that currently, LLMs have a low limit on "output tokens" and cannot transcribe the content of an entire article.
- Therefore, when we split the article into smaller fragments, we still want to retain information about their relation to the original. And that's why we need an identifier.

Another example is citing sources, which is useful, for instance, when the model is connected to the Internet. Besides statements, from the user's point of view, it is also valuable to include links to the websites the model uses.
## Providing Context for the Model

"Model APIs are stateless" â€” this sentence seems obvious and justifies the need to forward **the entire conversation content** to the model each time. However, this is not a solution for all problems, as seen in the following example.

We have here a message exchange where the user asks, who is overment. The system then searches the Internet, loading information into the context based on which it provides an answer. However, **if the context of the search results is removed from the conversation**, the next follow-up question will be addressed incorrectly.

![](https://cloud.overment.com/2024-09-10/aidevs3_state-e28e27c6-6.png)

A solution quickly comes to mind, which involves retaining the search results content in the conversation context. However, in practice, this is rarely possible because the number of tokens increases rapidly, and the model diverts attention from the original instructions. A much better approach includes the possibility of re-searching with a cache mechanism activated for a few to several minutes.

Therefore, the problem of providing knowledge to the context arises due to its very presence. Programmatically, we must ensure that the knowledge required to provide an answer is currently available to the model. However, this is not the end of the issues, as different sources of knowledge will interconnect and complement each other.

Even an innocent question like "find everything you know about me on the Internet" becomes less obvious because the model doesn't know by default what "about me" means and must first read our profile to generate search queries based on it.

![](https://cloud.overment.com/2024-09-10/aidevs3_loading-df70f143-b.png)

The above situation happens practically at every step. Below we see an example of a simple request to turn on favorite music, which indeed activates it. Similarly, we could ask for music to improve mood, facilitate focus, or for a night drive. In each case, the pattern is similar.

![](https://cloud.overment.com/2024-09-10/aidevs3_music-df9dde4c-b.png)

Although the user cannot see it, several additional actions occurred in the background. First and foremost, 2 of the dozens of actions that may be useful in this case were selected. In addition to names, the system generated commands related to how to activate them.

![](https://cloud.overment.com/2024-09-10/aidevs3_skills-3e6453d0-0.png)

Then the assistant asked a few questions to thoroughly scan its memory for information about favorite music. Behind the scenes, actions related to classifying these queries occurred, which meant each pertained to different areas of the assistant's memory. I mention them because the process of recalling memories is much more complex than listing a few questions, and we will see this in the later part of the course.

![](https://cloud.overment.com/2024-09-10/aidevs3_recall-14eac580-8.png)

Based on the gathered information, the system decided to make a request to the Spotify API, passing a list of potential tracks that might appeal to me. As a result, the music was started, and the system generated a confirmation visible to the user.

![](https://cloud.overment.com/2024-09-10/aidevs3_play-f4035209-1.png)

Translating this into visualization, a query requiring the use of many tools in an appropriate order looks as follows:

- The user's query is analyzed to create a plan and ask probing questions.
- Additional context is automatically retrieved from the database as a result of the search.
- The gathered information is passed to an external API.
- Depending on the API response, the user receives a confirmation message about the music being played. If an error occurs, retries are made to turn it on.

![](https://cloud.overment.com/2024-09-10/aidevs3_schema-67265988-6.png)

The `Spotify` example was already discussed in previous editions of AI_devs, but there we talked about fairly direct use of a specific tool. Here, however, we are dealing with a system capable of planning its actions, with the ability to respond to unforeseen situations, which it can usually handle independently.

The above scheme illustrates how important it is to control data flow at different stages of logic. Similar to programming functions, we don't always have to process all available data but select only what we need at the given moment. Here the situation is more complex, as we must master the non-deterministic nature of models.
## Summary

We discussed the physical delivery of context to the prompt in lesson S00E02 â€” Prompt Engineering. But now we see that this context will almost always be delivered programmatically and will come from various sources or be generated from scratch by the model. Hence, it is worth asking questions about which data we will use and how we will process it.

It is worth getting acquainted with the example [`files`](https://github.com/i-am-alice/3rd-devs/tree/main/files) to see how data can be used by the model during interaction. It is equally important how the model can transform content to use it more easily in the future.

If you use applications like Todoist, Clickup, or Linear, also try to replicate the mechanism of assigning new tasks to projects, just like the [`linear`](https://github.com/i-am-alice/3rd-devs/tree/main/linear) example shows. Besides assigning a task to a project, it is also possible to complete or enrich its description based on search results on the Internet or oneâ€™s own knowledge base.
Finally, looking at the examples discussed in this lesson, one can answer the question: **in what way can providing your own context help in optimizing the task [...]** (here insert an activity from your daily routine).